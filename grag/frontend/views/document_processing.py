"""
Document Processing Page View
æ–‡æª”è™•ç†é é¢ï¼Œä¸»è¦æ¥­å‹™é‚è¼¯çš„è¦–åœ–çµ„åˆ
"""
import asyncio
import streamlit as st
import tempfile
from pathlib import Path
from typing import Dict, Any, List
from grag.frontend.components import FileUpload, ProcessingDisplay
# from grag.frontend.services import FileProcessingService  # TODO: å¯¦ä½œæ­¤æœå‹™
from grag.frontend.utils import MAX_FILES, SUPPORTED_FORMATS, format_batch_results, display_metrics_grid
from grag.ingestion.indexing.ingestion_service import IngestionService

def render_document_processing_page(config: Dict[str, Any]):
    """
    æ¸²æŸ“æ–‡æª”è™•ç†é é¢

    Args:
        config: æ‡‰ç”¨é…ç½®å­—å…¸
    """
    # åˆå§‹åŒ–çµ„ä»¶
    upload_component = FileUpload()
    processing_display = ProcessingDisplay()

    # ç²å–é…ç½®
    vlm_strategy = config.get('vlm_strategy', 'è‡ªå‹•åˆ¤æ–·')
    force_vlm = config.get('force_vlm', None)
    embedding_provider = config.get('embedding_provider', 'sentence_transformers')

    # æ–‡ä»¶ä¸Šå‚³å€åŸŸ
    col1, col2 = st.columns([2, 1])

    with col1:
        st.markdown("### ğŸ“¤ æ–‡ä»¶ä¸Šå‚³æ¨¡å¼")

        # é¸æ“‡ä¸Šå‚³æ¨¡å¼
        upload_mode = st.radio(
            "é¸æ“‡ä¸Šå‚³é¡å‹", ["å–®å€‹æ–‡ä»¶", "æ‰¹é‡ä¸Šå‚³"],
            key="upload_mode_radio",
            help="å–®å€‹æ–‡ä»¶: ä¸Šå‚³ä¸€å€‹æ–‡ä»¶é€²è¡Œè™•ç† | æ‰¹é‡ä¸Šå‚³: ä¸Šå‚³å¤šå€‹æ–‡ä»¶åŒæ™‚è™•ç†"
        )

    with col2:
        # é¡¯ç¤ºé…ç½®æ‘˜è¦
        st.markdown("### âš™ï¸ ç•¶å‰é…ç½®")
        st.info(f"""
        **VLMç­–ç•¥**: {vlm_strategy}
        **åµŒå…¥æ¨¡å‹**: {embedding_provider}
        **æ–‡ä»¶é™åˆ¶**: {MAX_FILES} å€‹
        """)

    # æ ¹æ“šæ¨¡å¼è™•ç†æ–‡ä»¶
    if upload_mode == "å–®å€‹æ–‡ä»¶":
        _handle_single_file_upload(vlm_strategy, force_vlm, embedding_provider)
    else:
        _handle_batch_file_upload(vlm_strategy, force_vlm, embedding_provider)

def _handle_single_file_upload(vlm_strategy: str, force_vlm: bool, embedding_provider: str):
    """è™•ç†å–®å€‹æ–‡ä»¶ä¸Šå‚³"""
    st.markdown("### ğŸ“„ å–®å€‹æ–‡ä»¶ä¸Šå‚³")

    # æ–‡ä»¶ä¸Šå‚³å™¨
    uploaded_file = st.file_uploader(
        "é¸æ“‡æ¸¬è©¦æ–‡æª”",
        type=SUPPORTED_FORMATS,
        help=f"æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {', '.join(SUPPORTED_FORMATS).upper()}",
        key="single_file_uploader"
    )

    if uploaded_file is not None:
        # é¡¯ç¤ºæ–‡ä»¶è³‡è¨Š
        _display_file_info(uploaded_file)

        # è™•ç†æŒ‰éˆ•
        if st.button("ğŸš€ é–‹å§‹è™•ç†", type="primary", use_container_width=True):
            _process_single_file(uploaded_file, force_vlm, embedding_provider)

def _handle_batch_file_upload(vlm_strategy: str, force_vlm: bool, embedding_provider: str):
    """è™•ç†æ‰¹é‡æ–‡ä»¶ä¸Šå‚³"""
    st.markdown("### ğŸ“‚ æ‰¹é‡æ–‡ä»¶ä¸Šå‚³")

    # æ‰¹é‡æ–‡ä»¶ä¸Šå‚³å™¨
    uploaded_files = st.file_uploader(
        "é¸æ“‡å¤šå€‹æ¸¬è©¦æ–‡æª”",
        type=SUPPORTED_FORMATS,
        accept_multiple_files=True,
        help=f"æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: {', '.join(SUPPORTED_FORMATS).upper()}ï¼Œä¸€æ¬¡æœ€å¤š {MAX_FILES} å€‹æ–‡ä»¶",
        key="batch_file_uploader"
    )

    if uploaded_files:
        if len(uploaded_files) > MAX_FILES:
            st.error(f"ğŸš« ä¸€æ¬¡æ€§æœ€å¤šåªèƒ½ä¸Šå‚³ {MAX_FILES} å€‹æ–‡ä»¶ã€‚æ‚¨ç›®å‰é¸æ“‡äº† {len(uploaded_files)} å€‹æ–‡ä»¶ã€‚")
            return

        st.success(f"ğŸ“‚ å·²é¸æ“‡ {len(uploaded_files)} å€‹æ–‡ä»¶")

        # è™•ç†æŒ‰éˆ•
        if st.button(f"ğŸš€ æ‰¹é‡è™•ç† {len(uploaded_files)} å€‹æ–‡ä»¶", type="primary", use_container_width=True):
            _process_batch_files(uploaded_files, force_vlm, embedding_provider)

def _display_file_info(uploaded_file):
    """é¡¯ç¤ºä¸Šå‚³æ–‡ä»¶çš„è³‡è¨Š"""
    filename = uploaded_file.name
    file_ext = Path(filename).suffix.lower()

    st.success(f"ğŸ“„ å·²é¸æ“‡: {filename}")

    # æ–‡ä»¶è³‡è¨Š
    from grag.frontend.utils import get_file_info
    file_info = get_file_info(uploaded_file)

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ğŸ“ å¤§å°", file_info['size_formatted'])
    with col2:
        st.metric("ğŸ·ï¸ æ ¼å¼", file_ext.upper())
    with col3:
        status = "âœ… æ”¯æ´" if file_info['is_supported'] else "âŒ ä¸æ”¯æ´"
        st.metric("ğŸ“‹ ç‹€æ…‹", status)

def _process_single_file(uploaded_file, force_vlm: bool, embedding_provider: str):
    """è™•ç†å–®å€‹æ–‡ä»¶"""
    # å‰µå»ºé€²åº¦é¡¯ç¤º
    progress_bar = st.progress(0, "åˆå§‹åŒ–è™•ç†...")
    status_text = st.empty()
    result_area = st.empty()

    try:
        # ä¿å­˜æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
        with st.spinner("æº–å‚™æ–‡ä»¶..."):
            import tempfile
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uploaded_file.name}") as tmp_file:
                tmp_file.write(uploaded_file.read())
                file_path = Path(tmp_file.name)

        progress_bar.progress(20, "åˆå§‹åŒ–è™•ç†æœå‹™...")

        # åˆå§‹åŒ–è™•ç†æœå‹™
        ingestion_service = IngestionService()

        progress_bar.progress(40, "é–‹å§‹æ–‡ä»¶è™•ç†...")

        # åŸ·è¡Œè™•ç†
        status_text.text("ğŸ¯ æ­£åœ¨è™•ç†æ–‡ä»¶...")

        # æª¢æŸ¥äº‹ä»¶å¾ªç’°ç‹€æ…‹
        try:
            asyncio.get_running_loop()
            # å¦‚æœèƒ½å¤ ç²å–åˆ°é‹è¡Œä¸­çš„å¾ªç’°ï¼Œå°±ä¸åšä»»ä½•äº‹
        except RuntimeError:
            # å¦‚æœäº‹ä»¶å¾ªç’°ä¸å­˜åœ¨ï¼Œä½¿ç”¨ nest_asyncio
            import nest_asyncio
            nest_asyncio.apply()

        start_time = asyncio.get_event_loop().time()

        # æ³¨æ„ï¼šé€™è£¡çš„å¯¦ç¾å¯èƒ½éœ€è¦æ ¹æ“šå¯¦éš›çš„ IngestionService API èª¿æ•´
        result = asyncio.run(ingestion_service.ingest_document_enhanced(
            file_path=file_path,
            force_vlm=force_vlm
            # embedding_provider åƒæ•¸å¯èƒ½éœ€è¦æ·»åŠ åˆ° API ä¸­
        ))

        processing_time = asyncio.get_event_loop().time() - start_time

        progress_bar.progress(100, "è™•ç†å®Œæˆï¼")

        # é¡¯ç¤ºçµæœ
        with result_area.container():
            if result.get("success"):
                st.success("ğŸ‰ è™•ç†æˆåŠŸï¼")

                # è™•ç†çµ±è¨ˆ
                metadata = result.get("metadata", {})
                chunks_created = metadata.get("chunks_created", 0)
                embeddings_created = metadata.get("embeddings_created", 0)

                col1, col2, col3 = st.columns(3)
                with col1:
                    st.metric("â±ï¸ è™•ç†æ™‚é–“", f"{processing_time:.1f}s")
                with col2:
                    st.metric("ğŸ“Š åˆ†å¡Šæ•¸", chunks_created)
                with col3:
                    st.metric("ğŸ§® å‘é‡æ•¸", embeddings_created)

                # è™•ç†æ¨¡å¡Šè³‡è¨Š
                strategy_used = result.get("strategy_used", {})
                if strategy_used:
                    st.markdown("#### ğŸ”§ è™•ç†æ¨¡å¡Š")
                    processing_modules = _format_processing_modules(strategy_used)
                    st.info(f"ğŸ› ï¸ **è™•ç†éˆ**: {processing_modules}")

                # åµŒå…¥è³‡è¨Š
                if embeddings_created > 0:
                    provider_name = metadata.get("embedding_provider", embedding_provider).upper()
                    dimension = metadata.get("embedding_dimension", "N/A")

                    # å˜—è©¦ç²å–å¯¦éš›çš„.embedding_dimensionä¿¡æ¯
                    if dimension == "N/A" and result.get("statistics", {}).get("embeddings", {}).get("dimension"):
                        dimension = result.get("statistics", {}).get("embeddings", {}).get("dimension")

                    st.info(f"ğŸ¤– **åµŒå…¥è³‡è¨Š**: ä½¿ç”¨ {provider_name} æ¨¡å‹ï¼Œ"
                           f"å‘é‡ç¶­åº¦: {dimension}ï¼Œç”Ÿæˆå‘é‡: {embeddings_created} å€‹")

            else:
                st.error(f"âŒ è™•ç†å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

        # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
        try:
            file_path.unlink(missing_ok=True)
        except:
            pass

    except Exception as e:
        st.error(f"âŒ è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    finally:
        # æ¸…ç†é€²åº¦é¡¯ç¤º
        progress_bar.empty()
        status_text.empty()

def _process_batch_files(uploaded_files: List, force_vlm: bool, embedding_provider: str):
    """æ‰¹é‡è™•ç†æ–‡ä»¶"""
    total_files = len(uploaded_files)

    # å‰µå»ºæ•´é«”é€²åº¦é¡¯ç¤º
    main_progress = st.progress(0, f"æº–å‚™è™•ç† {total_files} å€‹æ–‡ä»¶...")
    status_text = st.empty()
    results_summary = st.empty()

    # æ”¶é›†è™•ç†çµæœ
    successful_uploads = 0
    failed_uploads = 0
    processing_results = []

    try:
        # åˆå§‹åŒ–è™•ç†æœå‹™
        ingestion_service = IngestionService()

        for i, uploaded_file in enumerate(uploaded_files):
            filename = uploaded_file.name
            current_progress = i / total_files

            # æ›´æ–°é€²åº¦
            main_progress.progress(current_progress, f"è™•ç†æ–‡ä»¶ {i+1}/{total_files}: {filename}")
            status_text.text(f"æ­£åœ¨è™•ç†: {filename}")

            try:
                # ä¿å­˜æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
                with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{filename}") as tmp_file:
                    tmp_file.write(uploaded_file.read())
                    file_path = Path(tmp_file.name)

                # è™•ç†æ–‡ä»¶ - æª¢æŸ¥äº‹ä»¶å¾ªç’°
                try:
                    asyncio.get_running_loop()
                    # å¦‚æœèƒ½å¤ ç²å–åˆ°é‹è¡Œä¸­çš„å¾ªç’°ï¼Œå°±ä¸åšä»»ä½•äº‹
                except RuntimeError:
                    # å¦‚æœäº‹ä»¶å¾ªç’°ä¸å­˜åœ¨ï¼Œä½¿ç”¨ nest_asyncio
                    import nest_asyncio
                    nest_asyncio.apply()

                result = asyncio.run(ingestion_service.ingest_document_enhanced(
                    file_path=file_path,
                    force_vlm=force_vlm
                ))

                # è¨˜éŒ„çµæœ
                processing_results.append({
                    'filename': filename,
                    'success': result.get('success', False),
                    'metadata': result.get('metadata', {}),
                    'error': result.get('error')
                })

                if result.get('success'):
                    successful_uploads += 1
                    st.success(f"âœ… {filename} è™•ç†æˆåŠŸ")
                else:
                    failed_uploads += 1
                    st.error(f"âŒ {filename} è™•ç†å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

                # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
                try:
                    file_path.unlink(missing_ok=True)
                except:
                    pass

            except Exception as e:
                failed_uploads += 1
                processing_results.append({
                    'filename': filename,
                    'success': False,
                    'error': str(e)
                })
                st.error(f"âŒ {filename} è™•ç†ç•°å¸¸: {str(e)}")

        # æ›´æ–°æœ€çµ‚é€²åº¦
        main_progress.progress(1.0, "æ‰¹é‡è™•ç†å®Œæˆï¼")

        # é¡¯ç¤ºç¸½çµçµæœ
        with results_summary.container():
            st.markdown("### ğŸ“Š æ‰¹é‡è™•ç†çµæœç¸½çµ")

            # çµæœçµ±è¨ˆ
            formatted_results = format_batch_results(processing_results)
            display_metrics_grid({
                key: str(value) if not isinstance(value, str) else value
                for key, value in formatted_results.items()
            }, columns=2)

            if successful_uploads > 0:
                st.success(f"ğŸ‰ æ‰¹é‡ä¸Šå‚³å®Œæˆï¼æˆåŠŸè™•ç† {successful_uploads} å€‹æ–‡ä»¶ã€‚")
            else:
                st.error("âŒ æ²’æœ‰æˆåŠŸè™•ç†ä»»ä½•æ–‡ä»¶ã€‚")

    except Exception as e:
        st.error(f"âŒ æ‰¹é‡è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    finally:
        # æ¸…ç†é€²åº¦é¡¯ç¤º
        main_progress.empty()
        status_text.empty()

def _format_processing_modules(strategy_used: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–è™•ç†æ¨¡å¡Šè³‡è¨Šé¡¯ç¤º"""
    modules = []

    # è™•ç†ä¸»è¦ç­–ç•¥
    if strategy_used.get("vlm_used"):
        if strategy_used.get("vlm_success"):
            # VLMæˆåŠŸçš„æƒ…æ³
            processing_layer = strategy_used.get("processing_layer", "").lower()
            if processing_layer == "vlm":
                vlm_provider = strategy_used.get("vlm_provider", "unknown")
                if vlm_provider == "ollama":
                    modules.append("ğŸ“… Ollamaæœ¬åœ°VLM")
                elif vlm_provider == "openai":
                    modules.append("ğŸŒ OpenAIé›²ç«¯VLM")
                else:
                    modules.append("ğŸ¤– VLMæœå‹™")
            elif processing_layer == "mineru":
                modules.append("ğŸ“„ MinerU PDFè§£æå™¨")
            elif processing_layer == "ocr":
                modules.append("ğŸ“ Tesseract OCR")
            else:
                modules.append(f"âš¡ {processing_layer.upper()}")
        else:
            # VLMå¤±æ•—ï¼Œé¡¯ç¤ºfallback
            processing_layer = strategy_used.get("processing_layer", "").lower()
            modules.append(f"âš ï¸ VLMé™ç´šè‡³{processing_layer}")
    else:
        modules.append("ğŸ“ ç›´æ¥æ–‡å­—è™•ç†")

    # æ·»åŠ LangChain
    if strategy_used.get("langchain_loaded"):
        modules.append("ğŸ”— LangChain")

    # æ·»åŠ åˆ†å¡Šå’ŒåµŒå…¥
    modules.extend(["ğŸ“Š LlamaIndexåˆ†å¡Š", "ğŸ§® SentenceTransformersåµŒå…¥"])

    # çµ„åˆæˆéˆ
    return " â†’ ".join(modules)

#!/usr/bin/env python3
"""
LangChainå¢å¼·è™•ç†æ¸¬è©¦GUI

é€™å€‹Streamlitæ‡‰ç”¨ç”¨æ–¼æ¸¬è©¦å‰›å¯¦ç¾çš„LangChainå¢å¼·æ–‡æª”è™•ç†åŠŸèƒ½ï¼Œ
åŒ…æ‹¬æ–‡ä»¶è¼‰å…¥ã€VLMç­–ç•¥ã€é™ç´šè™•ç†ã€åˆ†å¡Šå’ŒåµŒå…¥ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

ä½¿ç”¨æ–¹å¼:
    cd grag/frontend/
    uv run streamlit run app.py
"""

import sys
import os
from pathlib import Path
import asyncio
import tempfile
from typing import Dict, Any, Optional
import time

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import streamlit as st
from grag.ingestion.indexing.ingestion_service import IngestionService
from grag.core.config import settings
from grag.core.database_services import DatabaseManager

# é…ç½®é é¢
st.set_page_config(
    page_title="ğŸ”— LangChainè™•ç†æ¸¬è©¦å™¨",
    page_icon="ğŸ”—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è¨­å®šä¸­æ–‡ç•Œé¢
st.markdown("""
<style>
    .stApp {
        font-family: 'Microsoft YaHei', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ä¸»è¦æ¨™é¡Œ
st.title("ğŸ”— LangChainå¢å¼·æ–‡æª”è™•ç†æ¸¬è©¦å™¨")
st.markdown("---")

# ç³»çµ±è™•ç†èƒ½åŠ›ç¸½è¦½
st.markdown("### ğŸ”„ ç³»çµ±è™•ç†èƒ½åŠ›ç‹€æ…‹")

# è™•ç†èƒ½åŠ›æª¢æŸ¥å‡½æ•¸
def check_processing_capability(capability_name: str, check_logic, settings_obj) -> dict:
    """æª¢æŸ¥è™•ç†èƒ½åŠ›ç‹€æ…‹çš„å‡½æ•¸"""
    status = {"name": capability_name, "status": "unknown", "details": ""}

    try:
        if capability_name == "å¤šæ¨¡æ…‹è™•ç†":
            # æª¢æŸ¥VLMè™•ç†å™¨ - å„ªå…ˆé †åº: Ollama > OpenAI > Qwen2VL
            vlm_status = "ä½¿ç”¨é™ç´šè™•ç† (MinerU + OCR)"
            available_services = []

            # æª¢æŸ¥Ollama (æœ€é«˜å„ªå…ˆç´š)
            if getattr(settings_obj, 'ollama_base_url', None):
                try:
                    import requests
                    response = requests.get(f"{settings_obj.ollama_base_url.replace('/v1', '')}/api/tags", timeout=3)
                    if response.status_code == 200:
                        vlm_status = f"Ollamaé‹è¡Œä¸­ (æ¨¡å‹: {getattr(settings_obj, 'ollama_model', 'unknown')})"
                        available_services.append("Ollama")
                except:
                    available_services.append("Ollama (ä¸å¯ç”¨)")

            # æª¢æŸ¥OpenAI (ç¬¬äºŒå„ªå…ˆç´š)
            if not available_services and getattr(settings_obj, 'openai_api_key', None) and getattr(settings_obj, 'openai_api_key', '').startswith('sk-'):
                try:
                    import requests
                    payload = {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "test"}], "max_tokens": 1}
                    response = requests.post(
                        "https://api.openai.com/v1/chat/completions",
                        headers={"Authorization": f"Bearer {settings_obj.openai_api_key}"},
                        json=payload, timeout=5
                    )
                    if response.status_code == 200:
                        vlm_status = "OpenAI GPT-4Vå¯ç”¨"
                        available_services.append("OpenAI GPT-4V")
                except:
                    available_services.append("OpenAI GPT-4V (APIä¸å¯ç”¨)")

            # æª¢æŸ¥Qwen2VL (æœ€ä½å„ªå…ˆç´š)
            if not available_services and getattr(settings_obj, 'qwen2vl_base_url', None):
                try:
                    import requests
                    # é‡å°Qwen2VLï¼Œå˜—è©¦ç°¡å–®çš„GETè«‹æ±‚ä¾†æª¢æŸ¥æœå‹™
                    response = requests.get(getattr(settings_obj, 'qwen2vl_base_url', ''), timeout=5)
                    if response.status_code == 200:
                        vlm_status = "Qwen2VLæœå‹™å¯ç”¨"
                        available_services.append("Qwen2VL")
                except:
                    vlm_status = "ç„¡VLMæœå‹™"

            # ç¸½æ˜¯å¯ç”¨çš„é™ç´šè™•ç†å™¨
            fallback_services = ["MinerU", "Tesseract OCR"]

            # çµ„åˆæœ€çµ‚ç‹€æ…‹
            all_processors = available_services + fallback_services
            status_emoji = "âœ…" if available_services else "âš ï¸"
            status["status"] = f"{status_emoji} {vlm_status} â†’ {' + '.join(all_processors)}"
            return status

        elif capability_name == "æ–‡æœ¬è™•ç†":
            # æª¢æŸ¥åŸºæœ¬æ–‡æœ¬è™•ç†èƒ½åŠ›
            status["status"] = "âœ… LangChain + LlamaIndex + SentenceTransformers"
            status["details"] = "æ”¯æŒ: .txt, .md, .docx, .pdf"
            return status

        # å…¶ä»–èƒ½åŠ›æª¢æŸ¥
        result = check_logic(settings_obj)
        if result:
            status["status"] = "âœ… å¯ç”¨"
        else:
            status["status"] = "âŒ ä¸å¯ç”¨"

    except Exception as e:
        status["status"] = f"âŒ æª¢æŸ¥å¤±æ•—: {str(e)[:30]}"
        status["details"] = str(e)

    return status

# æª¢æŸ¥è³‡æ–™åº«é€£ç·š
def check_database_connectivity(settings_obj):
    db_available = False
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            getattr(settings_obj, 'neo4j_uri'),
            auth=(getattr(settings_obj, 'neo4j_user'), getattr(settings_obj, 'neo4j_password'))
        )
        driver.verify_connectivity()
        driver.close()
        db_available = True

        # æª¢æŸ¥Supabase
        from supabase import create_client
        client = create_client(getattr(settings_obj, 'supabase_url'), getattr(settings_obj, 'supabase_key'))
        response = client.table('vectors').select('*').limit(1).execute()
        db_available = db_available and True

    except Exception as e:
        pass

    return db_available

# ç”Ÿæˆç³»çµ±è™•ç†å ±å‘Š (çµ¦LLM/é–‹ç™¼è€…)
def _generate_processing_report(result: dict, processing_trace: dict = None) -> dict:
    """ç”Ÿæˆè©³ç´°çš„è™•ç†å ±å‘Šçµ¦LLMæˆ–é–‹ç™¼è€…åˆ†æ"""

    report = {
        "final_processor": "",
        "processor_type": "",
        "vlm_attempted": False,
        "vlm_success": False,
        "fallback_chain": [],
        "error_details": [],
        "performance": {},
        "recommendations": []
    }

    try:
        # å¾è™•ç†è»Œè·¡ä¸­æå–æœ€çµ‚è™•ç†å™¨
        if processing_trace and "processing_chain" in processing_trace:
            for step in processing_trace["processing_chain"]:
                if step.get("stage") == "æ–‡æª”è™•ç†":
                    if "VLM" in step.get("module", ""):
                        report["final_processor"] = "VLMè¦–è¦ºèªè¨€æ¨¡å‹"
                        report["processor_type"] = "advanced"
                        report["vlm_success"] = True
                    elif "MinerU" in step.get("module", ""):
                        report["final_processor"] = "MinerU PDFè™•ç†å¼•æ“"
                        report["processor_type"] = "medium"
                        report["fallback_chain"].append("VLMå¤±æ•—é™ç´šåˆ°MinerU")
                    elif "OCR" in step.get("module", ""):
                        report["final_processor"] = "Tesseract OCRå¼•æ“"
                        report["processor_type"] = "basic"
                        report["fallback_chain"].append("VLM+MinerUå¤±æ•—é™ç´šåˆ°OCR")
                    elif "StructuredTextFallback" in step.get("module", ""):
                        report["final_processor"] = "çµæ§‹åŒ–æ–‡å­—è™•ç†"
                        report["processor_type"] = "text"

        # ç­–ç•¥ä¿¡æ¯åˆ†æ
        strategy_info = result.get("strategy_used", {})
        if strategy_info.get("vlm_used"):
            report["vlm_attempted"] = True
            if not strategy_info.get("vlm_success"):
                report["error_details"].append("âš ï¸ VLMå˜—è©¦å¤±æ•—ï¼Œä½¿ç”¨é™ç´šè™•ç†å™¨")

        # æ•ˆèƒ½åˆ†æ
        processing_time = result.get("processing_time", 0)
        report["performance"] = {
            "total_time": f"{processing_time:.2f}ç§’",
            "evaluation": "å„ªè‰¯" if processing_time < 10 else "ä¸€èˆ¬" if processing_time < 30 else "è¼ƒæ…¢"
        }

        # ç”Ÿæˆå»ºè­°
        if report["processor_type"] == "basic":
            report["recommendations"].append("ğŸ”§ å»ºè­°: å•Ÿå‹•VLMæœå‹™ (Ollamaæˆ–OpenAI) ä»¥ç²å¾—æ›´å¥½çš„è™•ç†å“è³ª")
        elif report["processor_type"] == "medium":
            report["recommendations"].append("âœ¨ å»ºè­°: MinerUæ•ˆæœè‰¯å¥½ï¼Œä½†å¯ä»¥è€ƒæ…®OCRæ”¹é€²")
        elif report["processor_type"] == "advanced":
            report["recommendations"].append("ğŸ¯ ç³»çµ±é‹ä½œæœ€ä½³ï¼VLMè¦–è¦ºåˆ†æå·²æˆåŠŸæ‡‰ç”¨")

        # å“è³ªè©•ä¼°å ±å‘Š
        quality_level = result.get("metadata", {}).get("quality_level", "unknown")
        if quality_level == "high":
            report["quality_assessment"] = "âœ… é«˜å“è³ªè™•ç†ï¼šä½¿ç”¨äº†é€²éšè¦–è¦ºåˆ†æ"
        elif quality_level == "medium":
            report["quality_assessment"] = "âš ï¸ ä¸­ç­‰å“è³ªè™•ç†ï¼šä½¿ç”¨äº†PDFè§£æå™¨æˆ–å…‰å­¸è­˜åˆ¥"
        else:
            report["quality_assessment"] = "ğŸ“„ åŸºç¤è™•ç†ï¼šä½¿ç”¨äº†æ–‡å­—åˆ†æ"

    except Exception as e:
        report["error_details"].append(f"ç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    return report

# é¡¯ç¤ºè™•ç†å ±å‘Š (çµ¦é–‹ç™¼è€…/LLM)
def _display_processing_report(report: dict):
    """åœ¨GUIä¸­é¡¯ç¤ºè©³ç´°çš„è™•ç†å ±å‘Š"""

    st.markdown("### ğŸ“‹ ç³»çµ±è™•ç†è©³æƒ…")

    # æœ€çµ‚è™•ç†å™¨è³‡è¨Š
    col1, col2 = st.columns(2)
    with col1:
        st.metric("æœ€çµ‚ä½¿ç”¨çš„è™•ç†å™¨", report.get("final_processor", "Unknown"))

    with col2:
        processor_type_display = {
            "advanced": "ğŸš€ é€²éšç´š",
            "medium": "âš¡ ä¸­ç´š",
            "basic": "ğŸ“„ åŸºç¤ç´š",
            "text": "ğŸ“ æ–‡å­—ç´š"
        }
        st.metric("è™•ç†å™¨ç­‰ç´š", processor_type_display.get(report.get("processor_type"), "æœªçŸ¥"))

    # VLMå˜—è©¦ç‹€æ…‹
    if report.get("vlm_attempted"):
        if report.get("vlm_success"):
            st.success("âœ… VLMæœå‹™: æˆåŠŸè™•ç†æ–‡ä»¶")
        else:
            st.error("âŒ VLMæœå‹™: è™•ç†å¤±æ•—ï¼Œå•Ÿå‹•é™ç´šæ©Ÿåˆ¶")
    else:
        st.info("â„¹ï¸ VLMæœå‹™: æœªå˜—è©¦ (æŒ‰ç­–ç•¥æ±ºå®š)")

    # é™ç´šéˆæ¢
    if report.get("fallback_chain"):
        st.markdown("#### ğŸ”„ é™ç´šè™•ç†éˆæ¢")
        for i, fallback_reason in enumerate(report.get("fallback_chain", []), 1):
            st.markdown(f"{i}. {fallback_reason}")

    # å“è³ªè©•ä¼°
    if report.get("quality_assessment"):
        st.markdown("#### ğŸ“Š å“è³ªè©•ä¼°")
        st.markdown(report["quality_assessment"])

    # æ•ˆèƒ½è©•ä¼°
    if report.get("performance"):
        st.markdown("#### âš¡ æ•ˆèƒ½è©•ä¼°")
        perf = report["performance"]
        st.markdown(f"- **è™•ç†æ™‚é–“**: {perf['total_time']}")
        st.markdown(f"- **æ•ˆèƒ½ç­‰ç´š**: {perf['evaluation']}")

    # éŒ¯èª¤è©³æƒ…
    if report.get("error_details"):
        st.markdown("#### âš ï¸ éŒ¯èª¤åŠè­¦å‘Š")
        for error in report.get("error_details", []):
            st.markdown(error)

    # ç³»çµ±å»ºè­°
    if report.get("recommendations"):
        st.markdown("#### ğŸ’¡ ç³»çµ±å»ºè­°")
        for rec in report.get("recommendations", []):
            st.markdown(rec)

# æ‰€æœ‰è™•ç†èƒ½åŠ›
capabilities = [
    check_processing_capability("å¤šæ¨¡æ…‹è™•ç†", None, settings),
    check_processing_capability("æ–‡æœ¬è™•ç†", None, settings),
    check_database_connectivity(settings),
]

# é¡¯ç¤ºè™•ç†èƒ½åŠ›
col1, col2, col3 = st.columns(3)
with col1:
    capability = capabilities[0]  # å¤šæ¨¡æ…‹è™•ç†
    st.success(f"ğŸ¨ {capability['name']}: {capability['status']}")

with col2:
    st.success("ğŸ“ æ–‡æœ¬è™•ç†: âœ… LangChain + LlamaIndex + SentenceTransformers")

with col3:
    db_ok = capabilities[2]
    if db_ok:
        st.success("ğŸ—ƒï¸ è³‡æ–™åº«: âœ… Neo4j + Supabaseé€£ç·šæˆåŠŸ")
    else:
        st.error("ğŸ—ƒï¸ è³‡æ–™åº«: âŒ é€£ç·šå¤±æ•—")

st.markdown("**è™•ç†å„ªå…ˆé †åºèªªæ˜**:")
st.info("ğŸ“‹ **æ–‡ä»¶è™•ç†å„ªå…ˆé †åº**:\n"
        "1. **VLMæœå‹™** (å¦‚æœé‹è¡Œ) â†’ Ollama/OpenAI/Qwen2VL\n"
        "2. **MinerU** â†’ å¦‚æœVLMå¤±æ•—æˆ–è·³é\n"
        "3. **Tesseract OCR** â†’ æœ€çµ‚é™ç´šé¸é …\n"
        "4. **æ–‡å­—è™•ç†** â†’ å°æ–¼.txt/.mdæ–‡ä»¶")

st.markdown("---")

st.markdown("---")

# å´é‚Šæ¬„é…ç½®
st.sidebar.title("âš™ï¸ è™•ç†é…ç½®")

# VLMç­–ç•¥é¸æ“‡
vlm_strategy = st.sidebar.selectbox(
    "ğŸ¯ VLMè™•ç†ç­–ç•¥",
    ["è‡ªå‹•åˆ¤æ–·", "å¼·åˆ¶é–‹å•Ÿ", "å¼·åˆ¶é—œé–‰"],
    help="""
    è‡ªå‹•åˆ¤æ–·: æ ¹æ“šæ–‡ä»¶é¡å‹æ™ºèƒ½é¸æ“‡ (.pdfä½¿ç”¨VLM, .txt/.mdç›´æ¥è™•ç†)
    å¼·åˆ¶é–‹å•Ÿ: å°æ‰€æœ‰æ–‡æª”éƒ½ä½¿ç”¨VLMè™•ç† (æœƒè§¸ç™¼é™ç´šé‚è¼¯)
    å¼·åˆ¶é—œé–‰: è·³éVLMï¼Œç›´æ¥ä½¿ç”¨çµæ§‹åŒ–æ–‡å­—è™•ç†
    """
)

# ç­–ç•¥å€’æ›
force_vlm_map = {
    "è‡ªå‹•åˆ¤æ–·": None,    # Noneä»£è¡¨è‡ªå‹•
    "å¼·åˆ¶é–‹å•Ÿ": True,    # å¼·åˆ¶ä½¿ç”¨VLM
    "å¼·åˆ¶é—œé–‰": False    # å¼·åˆ¶è·³éVLM
}
force_vlm = force_vlm_map[vlm_strategy]

# ç­–ç•¥èªªæ˜
st.sidebar.markdown("**ç­–ç•¥é‚è¼¯èªªæ˜:**")
if vlm_strategy == "è‡ªå‹•åˆ¤æ–·":
    st.sidebar.info("""
    ğŸ“‹ **æ–‡ä»¶è™•ç†é‚è¼¯**:
    - `.pdf`, `.docx` â†’ VLMè™•ç† (è¦–è¦ºåˆ†æ)
    - `.txt`, `.md` â†’ ç›´æ¥è™•ç† (LangChainè¼‰å…¥)
    - å…¶ä»–æ ¼å¼ â†’ VLMå„ªå…ˆ (æœªçŸ¥å…§å®¹è¼ƒå®‰å…¨)
    """)
elif vlm_strategy == "å¼·åˆ¶é–‹å•Ÿ":
    st.sidebar.info("""
    ğŸ”§ **å¼·åˆ¶VLMæ¨¡å¼**:
    - å°æ‰€æœ‰æ–‡ä»¶å˜—è©¦VLMè™•ç†
    - å¤±æ•—æ™‚è‡ªå‹•é™ç´šåˆ°çµæ§‹åŒ–æ–‡å­—åˆ†æ
    - é©åˆæ¸¬è©¦é™ç´šæ©Ÿåˆ¶
    """)
else:  # å¼·åˆ¶é—œé–‰
    st.sidebar.info("""
    ğŸ“ **ç›´æ¥è™•ç†æ¨¡å¼**:
    - è·³éVLMï¼Œç›´æ¥ä½¿ç”¨LangChainè¼‰å…¥
    - ä½¿ç”¨çµæ§‹åŒ–æ–‡å­—åˆ†æ
    - æœ€å¿«é€Ÿçš„è™•ç†æ–¹å¼
    """)

st.sidebar.markdown("---")

# ç³»çµ±ç‹€æ…‹æª¢æŸ¥
st.sidebar.markdown("### ğŸ” ç³»çµ±ç‹€æ…‹")

# æª¢æŸ¥LangChainå®‰è£
try:
    import langchain_community
    st.sidebar.success("âœ… LangChainå¯ç”¨")
except ImportError:
    st.sidebar.error("âŒ LangChainæœªå®‰è£")

# æª¢æŸ¥VLMé…ç½®
if settings.qwen2vl_api_key or settings.openai_api_key:
    st.sidebar.success("âœ… VLMæœå‹™å·²é…ç½®")
else:
    st.sidebar.warning("âš ï¸ VLMæœå‹™æœªé…ç½® (å°‡ä½¿ç”¨é™ç´šè™•ç†)")

# æª¢æŸ¥åµŒå…¥æœå‹™
try:
    from grag.ingestion.indexing.providers.embedding_providers import create_embedding_provider, list_available_providers
    # å˜—è©¦å‰µå»ºé è¨­providerä¾†æ¸¬è©¦æ˜¯å¦æ­£å¸¸
    provider = create_embedding_provider()
    is_available = provider.is_available()
    if is_available:
        st.sidebar.success(f"âœ… åµŒå…¥æœå‹™å¯ç”¨ ({provider.name})")
    else:
        st.sidebar.warning(f"âš ï¸ åµŒå…¥æœå‹™æœªå®Œå…¨é…ç½®")
except Exception as e:
    st.sidebar.error(f"âŒ åµŒå…¥æœå‹™ç•°å¸¸: {str(e)[:30]}...")

# æª¢æŸ¥è³‡æ–™åº«é€£ç·š
st.sidebar.markdown("#### ğŸ“Š è³‡æ–™åº«é€£ç·š")

# Neo4jé€£ç·šæ¸¬è©¦
neo4j_connected = False
try:
    from neo4j import GraphDatabase
    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_user, settings.neo4j_password)
    )
    driver.verify_connectivity()
    driver.close()
    neo4j_connected = True
    st.sidebar.success("âœ… Neo4jå·²é€£ç·š")
except Exception as e:
    st.sidebar.error("âŒ Neo4jé€£ç·šå¤±æ•—")
    st.sidebar.caption(f"éŒ¯èª¤: {str(e)[:50]}...")

# Supabaseé€£ç·šæ¸¬è©¦
supabase_connected = False
try:
    from supabase import create_client
    client = create_client(settings.supabase_url, settings.supabase_key)
    # æ¸¬è©¦é€£ç·š - ç›´æ¥å‘¼å«health checkæˆ–ç°¡å–®çš„é€£æ¥æ¸¬è©¦
    # ä½¿ç”¨storageæ¸¬è©¦ï¼Œå› ç‚ºé€šå¸¸éƒ½å¯ç”¨
    storage = client.storage
    supabase_connected = True
    st.sidebar.success("âœ… Supabaseå·²é€£ç·š")
except Exception:
    st.sidebar.error("âŒ Supabaseé€£ç·šå¤±æ•—")
    # ä¸è¦åœ¨UIé¡¯ç¤ºAPIKeyï¼Œæœƒå½±éŸ¿å®‰å…¨æ€§
    st.sidebar.caption("æª¢æŸ¥.envè¨­å®š")

# è³‡æ–™åº«æ•´é«”ç‹€æ…‹
if neo4j_connected and supabase_connected:
    st.sidebar.success("ğŸ‰ æ‰€æœ‰è³‡æ–™åº«æ­£å¸¸é€£ç·šï¼")
elif neo4j_connected or supabase_connected:
    st.sidebar.warning("âš ï¸ éƒ¨åˆ†è³‡æ–™åº«å¯é€£ç·š")
else:
    st.sidebar.error("âŒ æ‰€æœ‰è³‡æ–™åº«é€£ç·šå¤±æ•—")

st.sidebar.markdown("---")

# ä¸»é é¢
col1, col2 = st.columns([1, 2])

with col1:
    st.markdown("### ğŸ“¤ æ–‡ä»¶ä¸Šå‚³")

    # æ–‡ä»¶ä¸Šå‚³å™¨
    uploaded_file = st.file_uploader(
        "é¸æ“‡æ¸¬è©¦æ–‡æª”",
        type=["pdf", "docx", "txt", "md"],
        help="æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: PDF, Word, æ–‡å­—, Markdown",
        key="uploaded_file"
    )

    if uploaded_file is not None:
        st.success(f"ğŸ“„ å·²é¸æ“‡: {uploaded_file.name}")

        # é¡¯ç¤ºæ–‡ä»¶è³‡è¨Š
        file_info = {
            "æ–‡ä»¶å": uploaded_file.name,
            "å¤§å°": f"{uploaded_file.size/1024:.1f} KB",
            "æ ¼å¼": Path(uploaded_file.name).suffix,
        }

        st.json(file_info)

        # VLMç­–ç•¥é©ç”¨æ€§æç¤º
        file_ext = Path(uploaded_file.name).suffix.lower()
        strategy_hint = {
            '.pdf': "å°‡ä½¿ç”¨VLMè™•ç†ï¼Œå› ç‚ºPDFéœ€è¦è¦–è¦ºåˆ†æ",
            '.docx': "å°‡å˜—è©¦VLMè™•ç†ï¼Œå¯å°è¤‡é›œæ ¼å¼çš„æ–‡ä»¶åˆ†æ",
            '.txt': "å°‡ç›´æ¥è™•ç†ï¼Œå› ç‚ºæ–‡å­—æ ¼å¼é©åˆLangChainè¼‰å…¥",
            '.md': "å°‡ç›´æ¥è™•ç†ï¼Œå› ç‚ºMarkdowné©åˆçµæ§‹åŒ–è§£æ"
        }

        if file_ext in strategy_hint:
            if vlm_strategy == "è‡ªå‹•åˆ¤æ–·":
                st.info(f"ğŸ¯ {strategy_hint[file_ext]}")
            else:
                st.info(f"ğŸ”§ æ‰‹å‹•ç­–ç•¥: {vlm_strategy}")

        # è™•ç†æŒ‰éˆ•
        process_button = st.button("ğŸš€ é–‹å§‹è™•ç†", type="primary", use_container_width=True)

    else:
        st.info("è«‹ä¸Šå‚³ä¸€å€‹æ–‡ä»¶ä¾†é–‹å§‹æ¸¬è©¦")
        process_button = False

with col2:
    st.markdown("### ğŸ“Š è™•ç†çµæœ")

    # æª¢æŸ¥æ˜¯å¦å¯ä»¥è™•ç†
    if not (process_button and uploaded_file is not None):
        st.info("â¬…ï¸ è«‹å…ˆä¸Šå‚³æ–‡ä»¶ä¸¦é»æ“Šè™•ç†æŒ‰éˆ•")
    else:
        # å‰µå»ºé€²åº¦æ¢
        progress_bar = st.progress(0, "åˆå§‹åŒ–...")
        status_text = st.empty()
        result_area = st.empty()

        try:
            # ä¿å­˜ä¸Šå‚³æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uploaded_file.name}") as tmp_file:
                tmp_file.write(uploaded_file.read())
                file_path = Path(tmp_file.name)

            progress_bar.progress(10, "æ–‡ä»¶è¼‰å…¥ä¸­...")

            # åˆå§‹åŒ–è™•ç†æœå‹™
            status_text.text("ğŸ”§ åˆå§‹åŒ–LangChainè™•ç†æœå‹™...")
            progress_bar.progress(20, "åˆå§‹åŒ–æœå‹™...")

            ingestion_service = IngestionService()

            progress_bar.progress(30, "é–‹å§‹è™•ç†...")

            # åŸ·è¡Œå¢å¼·è™•ç†
            status_text.text(f"ğŸ¯ è™•ç†ä¸­ ({vlm_strategy})...")

            start_time = time.time()

            # é€™æ˜¯é—œéµ! ä½¿ç”¨æˆ‘å€‘å‰›å¯¦ç¾çš„å¢å¼·è™•ç†æ–¹æ³•
            result = asyncio.run(ingestion_service.ingest_document_enhanced(
                file_path=file_path,
                force_vlm=force_vlm
            ))

            processing_time = time.time() - start_time

            progress_bar.progress(100, "è™•ç†å®Œæˆ! ğŸ‰")

            # ç²å–è™•ç†è»Œè·¡
            processing_trace = result.get("processing_trace", {})

            # é¡¯ç¤ºçµæœ - ç²¾ç°¡ç‰ˆå¸ƒå±€
            with result_area.container():
                if result.get("success"):
                    # å·¦é‚ŠæˆåŠŸç‹€æ…‹ï¼Œå³é‚Šå±•é–‹è©³ç´°è³‡è¨Š
                    col_left, col_right = st.columns([1, 2])

                    with col_left:
                        # ç¶ è‰²æˆåŠŸå€åŸŸ
                        st.success("ğŸ‰ è™•ç†æˆåŠŸï¼")

                        # åŸºæœ¬çµ±è¨ˆæŒ‡æ¨™ (ç°¡å–®ç‰ˆ)
                        st.metric("è™•ç†æ™‚é–“", f"{processing_time:.1f}s")
                        st.metric("åˆ†å¡Šæ•¸", result.get("metadata", {}).get("chunks_created", 0))
                        st.metric("å‘é‡æ•¸", result.get("metadata", {}).get("embeddings_created", 0))

                        # è™•ç†ç‹€æ…‹æ‘˜è¦
                        metadata = result.get("metadata", {})
                        quality_level = metadata.get("quality_level", "unknown")
                        st.info(f"å“è³ªç­‰ç´š: **{quality_level.upper()}**")

                    with col_right:
                        # å±•é–‹è©³ç´°è³‡è¨Šå€åŸŸ
                        with st.expander("ğŸ“‹ è™•ç†è©³æƒ…", expanded=True):
                            # ç”Ÿæˆç³»çµ±è™•ç†å ±å‘Š
                            processing_report = _generate_processing_report(result, processing_trace)

                            # æœ€çµ‚è™•ç†å™¨
                            if processing_report["final_processor"]:
                                final_processor_name = processing_report["final_processor"]
                                processor_icons = {
                                    "VLMè¦–è¦ºèªè¨€æ¨¡å‹": "ğŸ¤–",
                                    "MinerU PDFè™•ç†å¼•æ“": "ğŸ“‘",
                                    "Tesseract OCRå¼•æ“": "ğŸ”",
                                    "çµæ§‹åŒ–æ–‡å­—è™•ç†": "ğŸ“„"
                                }
                                icon = processor_icons.get(final_processor_name, "âš™ï¸")
                                st.markdown(f"**{icon} æœ€çµ‚è™•ç†å™¨**: {final_processor_name}")

                            # ç­–ç•¥çµæœ
                            strategy_info = result.get("strategy_used", {})
                            vlm_used = strategy_info.get("vlm_used", False)
                            vlm_success = strategy_info.get("vlm_success", False)

                            if vlm_used:
                                if vlm_success:
                                    st.success("âœ… VLMè™•ç†æˆåŠŸæ‡‰ç”¨")
                                else:
                                    st.warning("âš ï¸ VLMå˜—è©¦å¾Œé™ç´š")

                        # å±•é–‹è™•ç†è»Œè·¡
                        with st.expander("ğŸ”„ è™•ç†è»Œè·¡", expanded=False):
                            if "processing_trace" in result:
                                trace = result["processing_trace"]
                                st.write(f"**æ–‡ä»¶é¡å‹**: {trace['file_type']}")
                                st.write(f"**ä½¿ç”¨æ¨¡çµ„**: {', '.join(trace['modules_used'])}")

                                for step in trace.get("processing_chain", []):
                                    with st.container():
                                        cols = st.columns([1, 3])
                                        with cols[0]:
                                            st.write(f"**{step['stage']}**")
                                            st.caption(step.get('module', ''))
                                        with cols[1]:
                                            st.caption(step.get('description', ''))
                                        st.divider()

                        # å±•é–‹è³‡æ–™åº«çµæœ
                        with st.expander("ğŸ’¾ å„²å­˜çµæœ", expanded=False):
                            if "stage_results" in result:
                                stage_results = result["stage_results"]

                                if "neo4j" in stage_results:
                                    neo4j_result = stage_results["neo4j"]
                                    if isinstance(neo4j_result, dict) and neo4j_result.get("success"):
                                        st.success(f"ğŸ—‚ï¸ Neo4j: {neo4j_result.get('document_created', 0)} å€‹æ–‡ä»¶, {neo4j_result.get('chunks_created', 0)} å€‹åˆ†å¡Š")
                                    else:
                                        st.error("Neo4jå„²å­˜å¤±æ•—")

                                if "pgvector" in stage_results:
                                    pv_result = stage_results["pgvector"]
                                    if isinstance(pv_result, dict) and pv_result.get("success"):
                                        st.success(f"ğŸ—‚ï¸ Supabase: {pv_result.get('vectors_ingested', 0)} å€‹å‘é‡")
                                    else:
                                        st.error("Supabaseå„²å­˜å¤±æ•—")

                        # é–‹ç™¼è€…æ¨¡å¼å±•é–‹å€å¡Š
                        with st.expander("ğŸ”¬ é–‹ç™¼è€…è³‡è¨Š", expanded=False):
                            # çµ±è¨ˆå®Œæ•´ç‰ˆ
                            if "statistics" in result:
                                st.json(result["statistics"])

                            # ç­–ç•¥è©³ç´°ä¿¡æ¯
                            strategy_info = result.get("strategy_used", {})
                            st.write("**ç­–ç•¥è³‡è¨Š:**")
                            st.json(strategy_info)

                            # å®Œæ•´è™•ç†å ±å‘Š
                            if processing_report:
                                _display_processing_report(processing_report)

                else:
                    st.error(f"âŒ è™•ç†å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

                    # é¡¯ç¤ºéŒ¯èª¤è©³æƒ…
                    with st.expander("éŒ¯èª¤è©³æƒ…", expanded=True):
                        st.code(result.get('error', ''), language='text')

        except Exception as e:
            st.error(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            st.code(str(e), language='text')

        finally:
            # æ¸…ç†é€²åº¦é¡¯ç¤º
            status_text.empty()
            progress_bar.empty()

            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            try:
                if 'file_path' in locals() and file_path.exists():
                    file_path.unlink()
            except:
                pass

# é é¢åº•éƒ¨ä¿¡æ¯
st.markdown("---")
st.markdown("""
### ğŸ“– ä½¿ç”¨èªªæ˜

1. **ä¸Šå‚³æ–‡ä»¶**: é¸æ“‡æ”¯æ´çš„æ–‡ä»¶æ ¼å¼ (.pdf, .docx, .txt, .md)
2. **é¸æ“‡ç­–ç•¥**: 
   - è‡ªå‹•åˆ¤æ–·ï¼šæ ¹æ“šæ–‡ä»¶é¡å‹æ™ºèƒ½é¸æ“‡VLMä½¿ç”¨
   - å¼·åˆ¶é–‹å•Ÿï¼šæ¸¬è©¦VLMå¤±æ•—æ™‚çš„é™ç´šæ©Ÿåˆ¶
   - å¼·åˆ¶é—œé–‰ï¼šåªä½¿ç”¨LangChainç›´æ¥è™•ç†
3. **é–‹å§‹è™•ç†**: ç³»çµ±æœƒé¡¯ç¤ºå®Œæ•´çš„è™•ç†æµç¨‹å’Œçµæœ
4. **æŸ¥çœ‹çµæœ**: åŒ…å«è™•ç†æ™‚é–“ã€çµ±è¨ˆæ•¸æ“šå’Œè©³ç´°åˆ†æ

### ğŸ¯ æ¸¬è©¦é‡é»

é€™GUIå°ˆé–€ç”¨æ–¼æ¸¬è©¦æˆ‘å€‘å‰›å¯¦ç¾çš„LangChainå¢å¼·åŠŸèƒ½ï¼š
- **å¤šæ ¼å¼æ”¯æ´**: LangChainè¼‰å…¥å™¨çš„èƒ½åŠ›
- **æ™ºèƒ½ç­–ç•¥**: VLMè‡ªå‹•é¸æ“‡é‚è¼¯
- **é™ç´šæ©Ÿåˆ¶**: VLMå¤±æ•—æ™‚çš„å‚™ç”¨è™•ç†
- **è™•ç†çµ±è¨ˆ**: å®Œæ•´çš„æ•ˆèƒ½å’Œå“è³ªç›£æ§

---
**é–‹ç™¼ä¸­åŠŸèƒ½**: ç›®å‰ç‚ºç¬¬ä¸€éšæ®µæ¸¬è©¦ï¼Œè‘—é‡è™•ç†ç®¡é“é©—è­‰ã€‚
""")

#!/usr/bin/env python3
"""
LangChainå¢å¼·è™•ç†æ¸¬è©¦GUI

é€™å€‹Streamlitæ‡‰ç”¨ç”¨æ–¼æ¸¬è©¦å‰›å¯¦ç¾çš„LangChainå¢å¼·æ–‡æª”è™•ç†åŠŸèƒ½ï¼Œ
åŒ…æ‹¬æ–‡ä»¶è¼‰å…¥ã€VLMç­–ç•¥ã€é™ç´šè™•ç†ã€åˆ†å¡Šå’ŒåµŒå…¥ç­‰æ ¸å¿ƒåŠŸèƒ½ã€‚

ä½¿ç”¨æ–¹å¼:
    cd grag/frontend/
    uv run streamlit run app.py
"""

import sys
import os
from pathlib import Path
import asyncio
import tempfile
from typing import Dict, Any, Optional
import time

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°è·¯å¾‘
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import streamlit as st
from grag.ingestion.indexing.ingestion_service import IngestionService
from grag.core.config import settings
from grag.core.database_services import DatabaseManager

# é…ç½®é é¢
st.set_page_config(
    page_title="ğŸ”— LangChainè™•ç†æ¸¬è©¦å™¨",
    page_icon="ğŸ”—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è¨­å®šä¸­æ–‡ç•Œé¢
st.markdown("""
<style>
    .stApp {
        font-family: 'Microsoft YaHei', sans-serif;
    }
</style>
""", unsafe_allow_html=True)

# ä¸»è¦æ¨™é¡Œ
st.title("ğŸ”— LangChainå¢å¼·æ–‡æª”è™•ç†æ¸¬è©¦å™¨")
st.markdown("---")

# ç³»çµ±è™•ç†èƒ½åŠ›ç¸½è¦½
st.markdown("### ğŸ”„ ç³»çµ±è™•ç†èƒ½åŠ›ç‹€æ…‹")

# è™•ç†èƒ½åŠ›æª¢æŸ¥å‡½æ•¸
def check_processing_capability(capability_name: str, check_logic, settings_obj) -> dict:
    """æª¢æŸ¥è™•ç†èƒ½åŠ›ç‹€æ…‹çš„å‡½æ•¸"""
    status = {"name": capability_name, "status": "unknown", "details": ""}

    try:
        if capability_name == "å¤šæ¨¡æ…‹è™•ç†":
            # æª¢æŸ¥VLMè™•ç†å™¨ - å„ªå…ˆé †åº: Ollama > OpenAI > Qwen2VL
            vlm_status = "ä½¿ç”¨é™ç´šè™•ç† (MinerU + OCR)"
            available_services = []

            # æª¢æŸ¥Ollama (æœ€é«˜å„ªå…ˆç´š)
            if getattr(settings_obj, 'ollama_base_url', None):
                try:
                    import requests
                    response = requests.get(f"{settings_obj.ollama_base_url.replace('/v1', '')}/api/tags", timeout=3)
                    if response.status_code == 200:
                        vlm_status = f"Ollamaé‹è¡Œä¸­ (æ¨¡å‹: {getattr(settings_obj, 'ollama_model', 'unknown')})"
                        available_services.append("Ollama")
                except:
                    available_services.append("Ollama (ä¸å¯ç”¨)")

            # æª¢æŸ¥OpenAI (ç¬¬äºŒå„ªå…ˆç´š)
            if not available_services and getattr(settings_obj, 'openai_api_key', None) and getattr(settings_obj, 'openai_api_key', '').startswith('sk-'):
                try:
                    import requests
                    payload = {"model": "gpt-3.5-turbo", "messages": [{"role": "user", "content": "test"}], "max_tokens": 1}
                    response = requests.post(
                        "https://api.openai.com/v1/chat/completions",
                        headers={"Authorization": f"Bearer {settings_obj.openai_api_key}"},
                        json=payload, timeout=5
                    )
                    if response.status_code == 200:
                        vlm_status = "OpenAI GPT-4Vå¯ç”¨"
                        available_services.append("OpenAI GPT-4V")
                except:
                    available_services.append("OpenAI GPT-4V (APIä¸å¯ç”¨)")

            # Qwen2VL cloud service removed - using local Ollama only
            # for better privacy, cost control, and performance

            # ç¸½æ˜¯å¯ç”¨çš„é™ç´šè™•ç†å™¨
            fallback_services = ["MinerU", "Tesseract OCR"]

            # çµ„åˆæœ€çµ‚ç‹€æ…‹
            all_processors = available_services + fallback_services
            status_emoji = "âœ…" if available_services else "âš ï¸"
            status["status"] = f"{status_emoji} {vlm_status} â†’ {' + '.join(all_processors)}"
            return status

        elif capability_name == "æ–‡æœ¬è™•ç†":
            # æª¢æŸ¥åŸºæœ¬æ–‡æœ¬è™•ç†èƒ½åŠ›
            status["status"] = "âœ… LangChain + LlamaIndex + SentenceTransformers"
            status["details"] = "æ”¯æŒ: .txt, .md, .docx, .pdf"
            return status

        # å…¶ä»–èƒ½åŠ›æª¢æŸ¥
        result = check_logic(settings_obj)
        if result:
            status["status"] = "âœ… å¯ç”¨"
        else:
            status["status"] = "âŒ ä¸å¯ç”¨"

    except Exception as e:
        status["status"] = f"âŒ æª¢æŸ¥å¤±æ•—: {str(e)[:30]}"
        status["details"] = str(e)

    return status

# æª¢æŸ¥è³‡æ–™åº«é€£ç·š
def check_database_connectivity(settings_obj):
    db_available = False
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            getattr(settings_obj, 'neo4j_uri'),
            auth=(getattr(settings_obj, 'neo4j_user'), getattr(settings_obj, 'neo4j_password'))
        )
        driver.verify_connectivity()
        driver.close()
        db_available = True

        # æª¢æŸ¥Supabase
        from supabase import create_client
        client = create_client(getattr(settings_obj, 'supabase_url'), getattr(settings_obj, 'supabase_key'))
        response = client.table('vectors').select('*').limit(1).execute()
        db_available = db_available and True

    except Exception as e:
        pass

    return db_available

# ç”Ÿæˆç³»çµ±è™•ç†å ±å‘Š (çµ¦LLM/é–‹ç™¼è€…)
def _generate_processing_report(result: dict, processing_trace: dict = None) -> dict:
    """ç”Ÿæˆè©³ç´°çš„è™•ç†å ±å‘Šçµ¦LLMæˆ–é–‹ç™¼è€…åˆ†æ"""

    report = {
        "final_processor": "",
        "processor_type": "",
        "vlm_attempted": False,
        "vlm_success": False,
        "fallback_chain": [],
        "error_details": [],
        "performance": {},
        "recommendations": []
    }

    try:
        # å¾è™•ç†è»Œè·¡ä¸­æå–æœ€çµ‚è™•ç†å™¨
        if processing_trace and "processing_chain" in processing_trace:
            for step in processing_trace["processing_chain"]:
                if step.get("stage") == "æ–‡æª”è™•ç†":
                    if "VLM" in step.get("module", ""):
                        report["final_processor"] = "VLMè¦–è¦ºèªè¨€æ¨¡å‹"
                        report["processor_type"] = "advanced"
                        report["vlm_success"] = True
                    elif "MinerU" in step.get("module", ""):
                        report["final_processor"] = "MinerU PDFè™•ç†å¼•æ“"
                        report["processor_type"] = "medium"
                        report["fallback_chain"].append("VLMå¤±æ•—é™ç´šåˆ°MinerU")
                    elif "OCR" in step.get("module", ""):
                        report["final_processor"] = "Tesseract OCRå¼•æ“"
                        report["processor_type"] = "basic"
                        report["fallback_chain"].append("VLM+MinerUå¤±æ•—é™ç´šåˆ°OCR")
                    elif "StructuredTextFallback" in step.get("module", ""):
                        report["final_processor"] = "çµæ§‹åŒ–æ–‡å­—è™•ç†"
                        report["processor_type"] = "text"

        # ç­–ç•¥ä¿¡æ¯åˆ†æ
        strategy_info = result.get("strategy_used", {})
        if strategy_info.get("vlm_used"):
            report["vlm_attempted"] = True
            if not strategy_info.get("vlm_success"):
                report["error_details"].append("âš ï¸ VLMå˜—è©¦å¤±æ•—ï¼Œä½¿ç”¨é™ç´šè™•ç†å™¨")

        # æ•ˆèƒ½åˆ†æ
        processing_time = result.get("processing_time", 0)
        report["performance"] = {
            "total_time": f"{processing_time:.2f}ç§’",
            "evaluation": "å„ªè‰¯" if processing_time < 10 else "ä¸€èˆ¬" if processing_time < 30 else "è¼ƒæ…¢"
        }

        # ç”Ÿæˆå»ºè­°
        if report["processor_type"] == "basic":
            report["recommendations"].append("ğŸ”§ å»ºè­°: å•Ÿå‹•VLMæœå‹™ (Ollamaæˆ–OpenAI) ä»¥ç²å¾—æ›´å¥½çš„è™•ç†å“è³ª")
        elif report["processor_type"] == "medium":
            report["recommendations"].append("âœ¨ å»ºè­°: MinerUæ•ˆæœè‰¯å¥½ï¼Œä½†å¯ä»¥è€ƒæ…®OCRæ”¹é€²")
        elif report["processor_type"] == "advanced":
            report["recommendations"].append("ğŸ¯ ç³»çµ±é‹ä½œæœ€ä½³ï¼VLMè¦–è¦ºåˆ†æå·²æˆåŠŸæ‡‰ç”¨")

        # å“è³ªè©•ä¼°å ±å‘Š
        quality_level = result.get("metadata", {}).get("quality_level", "unknown")
        if quality_level == "high":
            report["quality_assessment"] = "âœ… é«˜å“è³ªè™•ç†ï¼šä½¿ç”¨äº†é€²éšè¦–è¦ºåˆ†æ"
        elif quality_level == "medium":
            report["quality_assessment"] = "âš ï¸ ä¸­ç­‰å“è³ªè™•ç†ï¼šä½¿ç”¨äº†PDFè§£æå™¨æˆ–å…‰å­¸è­˜åˆ¥"
        else:
            report["quality_assessment"] = "ğŸ“„ åŸºç¤è™•ç†ï¼šä½¿ç”¨äº†æ–‡å­—åˆ†æ"

    except Exception as e:
        report["error_details"].append(f"ç”Ÿæˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")

    return report

# é¡¯ç¤ºè™•ç†å ±å‘Š (çµ¦é–‹ç™¼è€…/LLM)
def _display_processing_report(report: dict):
    """åœ¨GUIä¸­é¡¯ç¤ºè©³ç´°çš„è™•ç†å ±å‘Š"""

    st.markdown("### ğŸ“‹ ç³»çµ±è™•ç†è©³æƒ…")

    # æœ€çµ‚è™•ç†å™¨è³‡è¨Š
    col1, col2 = st.columns(2)
    with col1:
        st.metric("æœ€çµ‚ä½¿ç”¨çš„è™•ç†å™¨", report.get("final_processor", "Unknown"))

    with col2:
        processor_type_display = {
            "advanced": "ğŸš€ é€²éšç´š",
            "medium": "âš¡ ä¸­ç´š",
            "basic": "ğŸ“„ åŸºç¤ç´š",
            "text": "ğŸ“ æ–‡å­—ç´š"
        }
        st.metric("è™•ç†å™¨ç­‰ç´š", processor_type_display.get(report.get("processor_type"), "æœªçŸ¥"))

    # VLMå˜—è©¦ç‹€æ…‹
    if report.get("vlm_attempted"):
        if report.get("vlm_success"):
            st.success("âœ… VLMæœå‹™: æˆåŠŸè™•ç†æ–‡ä»¶")
        else:
            st.error("âŒ VLMæœå‹™: è™•ç†å¤±æ•—ï¼Œå•Ÿå‹•é™ç´šæ©Ÿåˆ¶")
    else:
        st.info("â„¹ï¸ VLMæœå‹™: æœªå˜—è©¦ (æŒ‰ç­–ç•¥æ±ºå®š)")

    # é™ç´šéˆæ¢
    if report.get("fallback_chain"):
        st.markdown("#### ğŸ”„ é™ç´šè™•ç†éˆæ¢")
        for i, fallback_reason in enumerate(report.get("fallback_chain", []), 1):
            st.markdown(f"{i}. {fallback_reason}")

    # å“è³ªè©•ä¼°
    if report.get("quality_assessment"):
        st.markdown("#### ğŸ“Š å“è³ªè©•ä¼°")
        st.markdown(report["quality_assessment"])

    # æ•ˆèƒ½è©•ä¼°
    if report.get("performance"):
        st.markdown("#### âš¡ æ•ˆèƒ½è©•ä¼°")
        perf = report["performance"]
        st.markdown(f"- **è™•ç†æ™‚é–“**: {perf['total_time']}")
        st.markdown(f"- **æ•ˆèƒ½ç­‰ç´š**: {perf['evaluation']}")

    # éŒ¯èª¤è©³æƒ…
    if report.get("error_details"):
        st.markdown("#### âš ï¸ éŒ¯èª¤åŠè­¦å‘Š")
        for error in report.get("error_details", []):
            st.markdown(error)

    # ç³»çµ±å»ºè­°
    if report.get("recommendations"):
        st.markdown("#### ğŸ’¡ ç³»çµ±å»ºè­°")
        for rec in report.get("recommendations", []):
            st.markdown(rec)

# æ‰€æœ‰è™•ç†èƒ½åŠ›
capabilities = [
    check_processing_capability("å¤šæ¨¡æ…‹è™•ç†", None, settings),
    check_processing_capability("æ–‡æœ¬è™•ç†", None, settings),
    check_database_connectivity(settings),
]

# é¡¯ç¤ºè™•ç†èƒ½åŠ›
col1, col2, col3 = st.columns(3)
with col1:
    capability = capabilities[0]  # å¤šæ¨¡æ…‹è™•ç†
    st.success(f"ğŸ¨ {capability['name']}: {capability['status']}")

with col2:
    st.success("ğŸ“ æ–‡æœ¬è™•ç†: âœ… LangChain + LlamaIndex + SentenceTransformers")

with col3:
    db_ok = capabilities[2]
    if db_ok:
        st.success("ğŸ—ƒï¸ è³‡æ–™åº«: âœ… Neo4j + Supabaseé€£ç·šæˆåŠŸ")
    else:
        st.error("ğŸ—ƒï¸ è³‡æ–™åº«: âŒ é€£ç·šå¤±æ•—")

st.markdown("**è™•ç†å„ªå…ˆé †åºèªªæ˜**:")
st.info("ğŸ“‹ **æ–‡ä»¶è™•ç†å„ªå…ˆé †åº**:\n"
        "1. **VLMæœå‹™** (å¦‚æœé‹è¡Œ) â†’ Ollamaæˆ–OpenAI\n"
        "2. **MinerU** â†’ å¦‚æœVLMå¤±æ•—æˆ–è·³é\n"
        "3. **Tesseract OCR** â†’ æœ€çµ‚é™ç´šé¸é …\n"
        "4. **æ–‡å­—è™•ç†** â†’ å°æ–¼.txt/.mdæ–‡ä»¶")

st.markdown("---")

st.markdown("---")

# å´é‚Šæ¬„é…ç½®
st.sidebar.title("âš™ï¸ è™•ç†é…ç½®")

# VLMç­–ç•¥é¸æ“‡
vlm_strategy = st.sidebar.selectbox(
    "ğŸ¯ VLMè™•ç†ç­–ç•¥",
    ["è‡ªå‹•åˆ¤æ–·", "å¼·åˆ¶é–‹å•Ÿ", "å¼·åˆ¶é—œé–‰"],
    help="""
    è‡ªå‹•åˆ¤æ–·: æ ¹æ“šæ–‡ä»¶é¡å‹æ™ºèƒ½é¸æ“‡ (.pdfä½¿ç”¨VLM, .txt/.mdç›´æ¥è™•ç†)
    å¼·åˆ¶é–‹å•Ÿ: å°æ‰€æœ‰æ–‡æª”éƒ½ä½¿ç”¨VLMè™•ç† (æœƒè§¸ç™¼é™ç´šé‚è¼¯)
    å¼·åˆ¶é—œé–‰: è·³éVLMï¼Œç›´æ¥ä½¿ç”¨çµæ§‹åŒ–æ–‡å­—è™•ç†
    """
)

# ç­–ç•¥å€’æ›
force_vlm_map = {
    "è‡ªå‹•åˆ¤æ–·": None,    # Noneä»£è¡¨è‡ªå‹•
    "å¼·åˆ¶é–‹å•Ÿ": True,    # å¼·åˆ¶ä½¿ç”¨VLM
    "å¼·åˆ¶é—œé–‰": False    # å¼·åˆ¶è·³éVLM
}
force_vlm = force_vlm_map[vlm_strategy]

# ç­–ç•¥èªªæ˜
st.sidebar.markdown("**ç­–ç•¥é‚è¼¯èªªæ˜:**")
if vlm_strategy == "è‡ªå‹•åˆ¤æ–·":
    st.sidebar.info("""
    ğŸ“‹ **æ–‡ä»¶è™•ç†é‚è¼¯**:
    - `.pdf`, `.docx` â†’ VLMè™•ç† (è¦–è¦ºåˆ†æ)
    - `.txt`, `.md` â†’ ç›´æ¥è™•ç† (LangChainè¼‰å…¥)
    - å…¶ä»–æ ¼å¼ â†’ VLMå„ªå…ˆ (æœªçŸ¥å…§å®¹è¼ƒå®‰å…¨)
    """)
elif vlm_strategy == "å¼·åˆ¶é–‹å•Ÿ":
    st.sidebar.info("""
    ğŸ”§ **å¼·åˆ¶VLMæ¨¡å¼**:
    - å°æ‰€æœ‰æ–‡ä»¶å˜—è©¦VLMè™•ç†
    - å¤±æ•—æ™‚è‡ªå‹•é™ç´šåˆ°çµæ§‹åŒ–æ–‡å­—åˆ†æ
    - é©åˆæ¸¬è©¦é™ç´šæ©Ÿåˆ¶
    """)
else:  # å¼·åˆ¶é—œé–‰
    st.sidebar.info("""
    ğŸ“ **ç›´æ¥è™•ç†æ¨¡å¼**:
    - è·³éVLMï¼Œç›´æ¥ä½¿ç”¨LangChainè¼‰å…¥
    - ä½¿ç”¨çµæ§‹åŒ–æ–‡å­—åˆ†æ
    - æœ€å¿«é€Ÿçš„è™•ç†æ–¹å¼
    """)

st.sidebar.markdown("---")

# é é¢é¸æ“‡å™¨
page = st.sidebar.selectbox(
    "ğŸ“ é¸æ“‡é é¢",
    ["æ–‡æª”è™•ç†", "è³‡æ–™åº«ç®¡ç†"],
    help="""
    æ–‡æª”è™•ç†: ä¸Šå‚³å’Œè™•ç†æ–‡ä»¶ï¼Œæ¸¬è©¦RAGç®¡é“
    è³‡æ–™åº«ç®¡ç†: æŸ¥çœ‹å’Œåˆªé™¤è³‡æ–™åº«å…§å®¹ï¼ŒNeo4jåœ–å½¢è¦–è¦ºåŒ–
    """
)

st.sidebar.markdown("---")

# ç³»çµ±ç‹€æ…‹æª¢æŸ¥
st.sidebar.markdown("### ğŸ” ç³»çµ±ç‹€æ…‹")

# æª¢æŸ¥LangChainå®‰è£
try:
    import langchain_community
    st.sidebar.success("âœ… LangChainå¯ç”¨")
except ImportError:
    st.sidebar.error("âŒ LangChainæœªå®‰è£")

# æª¢æŸ¥VLMé…ç½® - æª¢æŸ¥Ollamaå’ŒOpenAI
vlm_configured = False
if getattr(settings, 'ollama_base_url', None):
    vlm_configured = True
if getattr(settings, 'openai_api_key', None):
    vlm_configured = True

if vlm_configured:
    st.sidebar.success("âœ… VLMæœå‹™å·²é…ç½®")
else:
    st.sidebar.warning("âš ï¸ VLMæœå‹™æœªé…ç½® (å°‡ä½¿ç”¨é™ç´šè™•ç†)")

# æª¢æŸ¥åµŒå…¥æœå‹™
try:
    from grag.ingestion.indexing.providers.embedding_providers import create_embedding_provider, list_available_providers
    # å˜—è©¦å‰µå»ºé è¨­providerä¾†æ¸¬è©¦æ˜¯å¦æ­£å¸¸
    provider = create_embedding_provider()
    is_available = provider.is_available()
    if is_available:
        st.sidebar.success(f"âœ… åµŒå…¥æœå‹™å¯ç”¨ ({provider.name})")
    else:
        st.sidebar.warning(f"âš ï¸ åµŒå…¥æœå‹™æœªå®Œå…¨é…ç½®")
except Exception as e:
    st.sidebar.error(f"âŒ åµŒå…¥æœå‹™ç•°å¸¸: {str(e)[:30]}...")

# æª¢æŸ¥è³‡æ–™åº«é€£ç·š
st.sidebar.markdown("#### ğŸ“Š è³‡æ–™åº«é€£ç·š")

# Neo4jé€£ç·šæ¸¬è©¦
neo4j_connected = False
try:
    from neo4j import GraphDatabase
    driver = GraphDatabase.driver(
        settings.neo4j_uri,
        auth=(settings.neo4j_user, settings.neo4j_password)
    )
    driver.verify_connectivity()
    driver.close()
    neo4j_connected = True
    st.sidebar.success("âœ… Neo4jå·²é€£ç·š")
except Exception as e:
    st.sidebar.error("âŒ Neo4jé€£ç·šå¤±æ•—")
    st.sidebar.caption(f"éŒ¯èª¤: {str(e)[:50]}...")

# Supabaseé€£ç·šæ¸¬è©¦
supabase_connected = False
try:
    from supabase import create_client
    client = create_client(settings.supabase_url, settings.supabase_key)
    # æ¸¬è©¦é€£ç·š - ç›´æ¥å‘¼å«health checkæˆ–ç°¡å–®çš„é€£æ¥æ¸¬è©¦
    # ä½¿ç”¨storageæ¸¬è©¦ï¼Œå› ç‚ºé€šå¸¸éƒ½å¯ç”¨
    storage = client.storage
    supabase_connected = True
    st.sidebar.success("âœ… Supabaseå·²é€£ç·š")
except Exception:
    st.sidebar.error("âŒ Supabaseé€£ç·šå¤±æ•—")
    # ä¸è¦åœ¨UIé¡¯ç¤ºAPIKeyï¼Œæœƒå½±éŸ¿å®‰å…¨æ€§
    st.sidebar.caption("æª¢æŸ¥.envè¨­å®š")

# è³‡æ–™åº«æ•´é«”ç‹€æ…‹
if neo4j_connected and supabase_connected:
    st.sidebar.success("ğŸ‰ æ‰€æœ‰è³‡æ–™åº«æ­£å¸¸é€£ç·šï¼")
elif neo4j_connected or supabase_connected:
    st.sidebar.warning("âš ï¸ éƒ¨åˆ†è³‡æ–™åº«å¯é€£ç·š")
else:
    st.sidebar.error("âŒ æ‰€æœ‰è³‡æ–™åº«é€£ç·šå¤±æ•—")

st.sidebar.markdown("---")

# è³‡æ–™åº«ç®¡ç†é é¢å‡½æ•¸
def show_database_management_page():
    """é¡¯ç¤ºè³‡æ–™åº«ç®¡ç†é é¢ï¼ŒåŒ…å«Neo4j Browserå’Œè³‡æ–™ç€è¦½"""

    # æª¢æŸ¥æ˜¯å¦æœ‰æ•¸æ“šåº«è®Šæ›´é€šçŸ¥
    if st.session_state.get('database_modified', False):
        st.info("ğŸ”„ **æ•¸æ“šæ›´æ–°æç¤º**: æ•¸æ“šåº«æœ€è¿‘ç™¼ç”Ÿäº†è®ŠåŒ–ï¼ˆæ–‡ä»¶åˆ é™¤/ä¸Šå‚³ï¼‰ã€‚è«‹åˆ·æ–°é é¢æŸ¥çœ‹æœ€æ–°ç‹€æ…‹ã€‚")

        # æ·»åŠ ä¸€ä¸ªåˆ·æ–°æŒ‰é’®
        if st.button("ğŸ”„ åˆ·æ–°æ•°æ®åº“è§†å›¾", type="secondary", use_container_width=True):
            st.session_state.database_modified = False
            st.rerun()

        st.markdown("---")

    st.markdown("# ğŸ—ƒï¸ è³‡æ–™åº«ç®¡ç†")

    # Tabåˆ†é ï¼šNeo4jè¦–è¦ºåŒ–ã€Supabaseè³‡æ–™ã€åˆªé™¤æ¸¬è©¦
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸŒ Neo4j Browser",
        "ğŸ“Š Supabaseå‘é‡",
        "ğŸ—‚ï¸ æ–‡ä»¶ç®¡ç†",
        "ğŸ—‘ï¸ åˆªé™¤æ¸¬è©¦"
    ])

    with tab1:
        show_neo4j_browser()

    with tab2:
        show_supabase_vectors()

    with tab3:
        show_document_management()

    with tab4:
        show_deletion_tests()

def show_neo4j_browser():
    """Neo4j Browseré›†æˆ"""
    st.markdown("### ğŸ•¸ï¸ Neo4jåœ–å½¢è³‡æ–™åº«")

    # æª¢æŸ¥Neo4jå¯ç”¨æ€§
    neo4j_available = False
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            settings.neo4j_uri,
            auth=(settings.neo4j_user, settings.neo4j_password)
        )
        driver.verify_connectivity()
        driver.close()
        neo4j_available = True
    except Exception as e:
        st.error(f"âŒ Neo4jé€£ç·šå¤±æ•—: {str(e)[:50]}...")
        st.info("è«‹ç¢ºä¿Neo4jæœå‹™é‹è¡Œåœ¨ neo4j://localhost:7687")
        return

    if neo4j_available:
        st.success("âœ… Neo4jå·²é€£ç·š")
        st.markdown("""
        **Neo4j Browser** æä¾›å®Œæ•´çš„åœ–å½¢è³‡æ–™åº«è¦–è¦ºåŒ–å’ŒæŸ¥è©¢ä»‹é¢ã€‚

        ğŸ¯ å¸¸ç”¨æŸ¥è©¢ç¤ºä¾‹ï¼š
        ```cypher
        // æŸ¥çœ‹æ‰€æœ‰Documentç¯€é»
        MATCH (d:Document) RETURN d LIMIT 10

        // æŸ¥çœ‹Chunkå’Œå…¶é—œè¯
        MATCH (d:Document)-[:HAS_CHUNK]->(c:Chunk) RETURN d, c LIMIT 5

        // æŸ¥æ‰¾å¯¦é«”
        MATCH (e:Entity) RETURN e.name, e.type LIMIT 10
        ```
        """)

        # å¾è¨­å®šä¸­ç²å–Neo4j Browser URL
        neo4j_uri = getattr(settings, 'neo4j_uri', 'neo4j://localhost:7687')
        # Neo4j Browseré€šå¸¸åœ¨7687ç«¯å£åŸºç¤ä¸Š+11ï¼Œå³7474
        if '7687' in neo4j_uri:
            browser_port = '7474'
        else:
            browser_port = '7474'  # é è¨­å€¼

        # æå–hostå¾URI
        import re
        host_match = re.search(r'neo4j://([^:]+)', neo4j_uri)
        browser_host = host_match.group(1) if host_match else 'localhost'

        browser_url = f"http://{browser_host}:{browser_port}/browser/"

        st.success(f"ğŸ”— Neo4j Browseré‹è¡Œåœ¨: http://{browser_host}:{browser_port}/browser/")

        # æä¾›éˆæ¥åˆ°æ–°çª—å£è€Œä¸æ˜¯iframeï¼ˆé¿å…CORSå’ŒåµŒå…¥å•é¡Œï¼‰
        st.markdown("""
        **ğŸ–¥ï¸ Neo4j Browser åœ–å½¢åŒ–ä»‹é¢**
        """)
        col1, col2 = st.columns(2)
        with col1:
            if st.button("ğŸŒ åœ¨æ–°çª—å£æ‰“é–‹Neo4j Browser", type="primary"):
                st.markdown(f'<meta http-equiv="refresh" content="0; url=http://{browser_host}:{browser_port}/browser/">', unsafe_allow_html=True)
        with col2:
            st.markdown("æˆ–è€…æ‰‹å‹•è¨ªå•:")
            st.code(f"http://{browser_host}:{browser_port}/browser/")

        # æ·»åŠ ä¸€å€‹èªªæ˜å€åŸŸ
        with st.expander("â„¹ï¸ é€£ç·šè³‡è¨Š", expanded=False):
            st.markdown(f"""
            - **Neo4j Browser URL**: `http://{browser_host}:{browser_port}/browser/`
            - **ç”¨æˆ¶å**: neo4j
            - **å¯†ç¢¼**: testpass123
            - **BOLTé€£ç·š**: neo4j://{browser_host}:7687
            """)

            st.info("ğŸ’¡ å¦‚æœé‡åˆ°é€£ç·šå•é¡Œï¼Œè«‹æª¢æŸ¥Neo4jå®¹å™¨æ˜¯å¦æ­£ç¢ºå•Ÿå‹•")

        # æä¾›iframeä½œç‚ºå‚™ç”¨é¸é …ï¼Œä½†æœ‰è­¦å‘Š
        with st.expander("ğŸ”§ åµŒå…¥å¼æª¢è¦– (å¯èƒ½ä¸æ”¯æŒiframe)", expanded=False):
            st.warning("âš ï¸ Neo4j Browserå¯èƒ½ä¸æ”¯æŒiframeåµŒå…¥ï¼Œå¦‚æœçœ‹ä¸åˆ°å…§å®¹ï¼Œè«‹ä½¿ç”¨ä¸Šé¢çš„éˆæ¥åœ¨æ–°çª—å£æ‰“é–‹")

            # HTML iframe with broader permissions
            iframe_html = f"""
            <iframe src="{browser_url}"
                    style="width:100%; height:600px; border:1px solid #ddd;"
                    sandbox="allow-scripts allow-forms allow-same-origin allow-popups allow-presentation"
                    allowfullscreen>
            </iframe>
            """
            st.components.v1.html(iframe_html, height=600)

        st.markdown("""
        ---
        **ğŸ’¡ ä½¿ç”¨æç¤º:**
        - å³éµç¯€é»å¯å±•é–‹æ›´å¤šè³‡è¨Š
        - ä½¿ç”¨åœ–å½¢è¦–è¦ºåŒ–æŸ¥çœ‹å¯¦é«”é—œä¿‚
        - æ”¯æ´å®Œæ•´CypheræŸ¥è©¢èªè¨€
        - é©åˆé–‹ç™¼éšæ®µçš„è³‡æ–™æ¢ç´¢å’Œèª¿è©¦
        """)
    else:
        st.error("Neo4jæœå‹™ä¸å¯ç”¨ï¼Œç„¡æ³•è¼‰å…¥Browserä»‹é¢")

def show_supabase_vectors():
    """é¡¯ç¤ºSupabaseå‘é‡è³‡æ–™"""
    st.markdown("### ğŸ—ƒï¸ Supabaseå‘é‡è³‡æ–™åº«")

    try:
        from supabase import create_client
        client = create_client(settings.supabase_url, settings.supabase_key)

        # ç²å–åŸºæœ¬çµ±è¨ˆ
        response = client.table('vectors').select('*', count='exact').execute()

        total_vectors = response.count if hasattr(response, 'count') else len(response.data)
        st.metric("ç¸½å‘é‡æ•¸", total_vectors)

        # é¡¯ç¤ºæœ€è¿‘çš„å‘é‡
        recent_response = client.table('vectors').select(
            'vector_id, document_id, chunk_id, fact_id, type, page, order, created_at'
        ).order('created_at', desc=True).limit(20).execute()

        if recent_response.data:
            import pandas as pd
            df = pd.DataFrame(recent_response.data)

            # æ ¼å¼åŒ–é¡¯ç¤º
            df['vector_id'] = df['vector_id'].str[:8] + '...'
            df['document_id'] = df['document_id'].str[:8] + '...'
            df['chunk_id'] = df['chunk_id'].apply(lambda x: (x[:8] + '...') if x else 'N/A')
            df['fact_id'] = df['fact_id'].apply(lambda x: (x[:8] + '...') if x else 'N/A')

            st.dataframe(df, use_container_width=True)

            # ä¸‹è¼‰æŒ‰éˆ•
            csv = df.to_csv(index=False)
            st.download_button(
                label="ğŸ“¥ ä¸‹è¼‰ç‚ºCSV",
                data=csv,
                file_name="vectors_export.csv",
                mime="text/csv"
            )
        else:
            st.info("ç›®å‰æ²’æœ‰å‘é‡è³‡æ–™")

    except Exception as e:
        st.error(f"ç„¡æ³•è¼‰å…¥Supabaseè³‡æ–™: {str(e)[:100]}...")
        st.info("è«‹æª¢æŸ¥.envä¸­çš„Supabaseè¨­å®š")

def show_document_management():
    """æ–‡ä»¶ç®¡ç† - åˆ—å‡ºæ‰€æœ‰å·²è™•ç†çš„æ–‡ä»¶"""
    st.markdown("### ğŸ“„ å·²è™•ç†æ–‡ä»¶ç®¡ç†")

    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            settings.neo4j_uri,
            auth=(settings.neo4j_user, settings.neo4j_password)
        )

        with driver.session() as session:
            # ç²å–æ‰€æœ‰Document
            result = session.run("""
            MATCH (d:Document)
            OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)
            RETURN d.document_id as id, d.title as title, d.source_path as path,
                   count(c) as chunks, d.hash as hash,
                   d.created_at as created_at, d.updated_at as updated_at
            ORDER BY d.created_at DESC
            """)

            # æ­£ç¢ºè½‰æ›Neo4jç´€éŒ„ç‚ºå­—å…¸åˆ—è¡¨
            documents = []
            for record in result:
                documents.append({
                    'id': record['id'],
                    'title': record['title'],
                    'path': record['path'],
                    'chunks': record['chunks'],
                    'hash': record['hash'],
                    'created_at': record['created_at'],
                    'updated_at': record['updated_at']
                })

        if documents:
            import pandas as pd
            df = pd.DataFrame(documents)

            # æ ¼å¼åŒ–è·¯å¾‘é¡¯ç¤º
            df['path'] = df['path'].apply(lambda x: Path(x).name if x else 'Unknown')

            # é‡æ–°å‘½åæ¬„ä½
            df.columns = ['æ–‡ä»¶ID', 'æ¨™é¡Œ', 'æ–‡ä»¶å', 'åˆ†å¡Šæ•¸', 'Hash', 'å‰µå»ºæ™‚é–“', 'æ›´æ–°æ™‚é–“']

            st.dataframe(df, use_container_width=True)
            st.success(f"ç¸½å…± {len(documents)} å€‹å·²è™•ç†çš„æ–‡ä»¶")
        else:
            st.info("ç›®å‰æ²’æœ‰å·²è™•ç†çš„æ–‡ä»¶")

        driver.close()

    except Exception as e:
        st.error(f"ç„¡æ³•è¼‰å…¥æ–‡ä»¶è³‡æ–™: {str(e)[:100]}...")
        st.info("è«‹æª¢æŸ¥Neo4jé€£ç·š")

def show_deletion_tests():
    """åˆªé™¤æ¸¬è©¦åŠŸèƒ½"""
    st.markdown("### ğŸ—‘ï¸ è³‡æ–™åº«åˆªé™¤æ¸¬è©¦")
    st.warning("âš ï¸ åˆªé™¤æ“ä½œä¸å¯é€†ï¼Œè«‹è¬¹æ…ä½¿ç”¨")

    # é¸æ“‡åˆªé™¤é¡å‹
    delete_type = st.selectbox(
        "é¸æ“‡åˆªé™¤æ¸¬è©¦é¡å‹",
        ["é¸æ“‡é¡å‹", "Documentåˆªé™¤"],
        help="""
        Documentåˆªé™¤: æ¸¬è©¦å®Œæ•´æ–‡ä»¶åˆªé™¤ (åŒ…å«æ‰€æœ‰é—œè¯çš„chunkså’Œvectors)
        å…¶ä»–åˆªé™¤åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­...
        """
    )

    if delete_type != "é¸æ“‡é¡å‹":
        st.markdown("---")

        # æ ¹æ“šé¡å‹é¡¯ç¤ºä¸åŒçš„é¸æ“‡ä»‹é¢
        if delete_type == "Documentåˆªé™¤":
            show_document_deletion_test()
        # æš‚æ—¶æ³¨é‡Šæ‰æœªå®Œæˆçš„åŠŸèƒ½
        # elif delete_type == "Chunkåˆªé™¤":
        #     show_chunk_deletion_test()
        # elif delete_type == "Visual Factåˆªé™¤":
        #     show_visual_fact_deletion_test()

    # é¡¯ç¤ºæœªå®Œæˆçš„åŠŸèƒ½é€šçŸ¥å€åŸŸ
    if st.checkbox("é¡¯ç¤ºé–‹ç™¼ä¸­çš„åŠŸèƒ½", key="show_dev_features"):
        with st.expander("âš ï¸ é–‹ç™¼ä¸­çš„åŠŸèƒ½ (è«‹è¬¹æ…ä½¿ç”¨)", expanded=False):
            st.warning("ä»¥ä¸‹åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­ï¼Œå¯èƒ½ä¸ç©©å®šæˆ–ä¸å®Œæ•´ã€‚")

            dev_delete_type = st.radio(
                "é¸æ“‡é–‹ç™¼ä¸­åŠŸèƒ½",
                ["ä¸é¸æ“‡", "Chunkåˆªé™¤æ¸¬è©¦", "Visual Factåˆªé™¤æ¸¬è©¦"],
                help="é€™äº›åŠŸèƒ½å°šæœªå®Œæˆï¼Œå¯èƒ½æœƒæœ‰å•é¡Œã€‚",
                key="dev_delete_radio"
            )

            if dev_delete_type == "Chunkåˆªé™¤æ¸¬è©¦":
                show_chunk_deletion_test()
            elif dev_delete_type == "Visual Factåˆªé™¤æ¸¬è©¦":
                show_visual_fact_deletion_test()

def show_document_deletion_test():
    """Documentåˆªé™¤æ¸¬è©¦ - æ”¯æ´æ‰¹é‡åˆªé™¤"""
    st.markdown("#### ğŸ“„ Documentæ‰¹é‡åˆªé™¤æ¸¬è©¦")

    try:
        from neo4j import GraphDatabase
        from uuid import UUID
        driver = GraphDatabase.driver(
            settings.neo4j_uri,
            auth=(settings.neo4j_user, settings.neo4j_password)
        )

        with driver.session() as session:
            result = session.run("""
            MATCH (d:Document)
            OPTIONAL MATCH (d)-[:HAS_CHUNK]->(c:Chunk)
            RETURN d.document_id as id, d.title as title, count(c) as chunks,
                   d.source_path as source_path, d.created_at as created_at
            ORDER BY d.created_at DESC
            """)
            documents = []
            for record in result:
                documents.append({
                    'id': record['id'],
                    'title': record['title'],
                    'chunks': record['chunks'],
                    'source_path': record['source_path'] or '',
                    'created_at': record['created_at']
                })

        driver.close()

        if documents:
            st.markdown(f"**ç¸½å…±æœ‰ {len(documents)} å€‹æ–‡ä»¶**")

            # ä½¿ç”¨session stateä¾†è¿½è¹¤é¸ä¸­çš„é …ç›®
            if 'selected_documents' not in st.session_state:
                st.session_state.selected_documents = []

            # å…¨é¸/å–æ¶ˆå…¨é¸æŒ‰éˆ•
            col1, col2, col3 = st.columns([2, 2, 3])
            with col1:
                if st.button("âœ… å…¨é¸", key="select_all_button", use_container_width=True):
                    st.session_state.selected_documents = [doc['id'] for doc in documents]
                    st.rerun()  # ç«‹å³é‡æ–°è¼‰å…¥ä»¥æ›´æ–°ä»‹é¢

            with col2:
                if st.button("âŒ å–æ¶ˆå…¨é¸", key="clear_selection_button", use_container_width=True):
                    st.session_state.selected_documents = []
                    st.rerun()  # ç«‹å³é‡æ–°è¼‰å…¥ä»¥æ›´æ–°ä»‹é¢

            with col3:
                st.markdown(f"**å·²é¸æ“‡: {len(st.session_state.selected_documents)} å€‹**")

            # é¡¯ç¤ºæ–‡ä»¶åˆ—è¡¨ï¼Œæ¯å€‹éƒ½æœ‰checkbox
            st.markdown("### ğŸ“‹ æ–‡ä»¶åˆ—è¡¨")
            st.info("ğŸ’¡ **æ“ä½œæç¤º**: å‹¾é¸æ–‡ä»¶æ™‚é é¢æœƒçŸ­æš«åˆ·æ–°ï¼Œé€™æ˜¯æ­£å¸¸è¡Œç‚ºï¼Œå¯ç«‹å³çœ‹åˆ°é¸æ“‡ç‹€æ…‹")

            with st.spinner("è¼‰å…¥æ–‡ä»¶åˆ—è¡¨ä¸­..."):
                selected_count = 0

                for doc in documents:
                    is_selected = doc['id'] in st.session_state.selected_documents

                    # å‰µå»ºæ¯è¡Œçš„checkbox
                    col_checkbox, col_info = st.columns([1, 11])

                    with col_checkbox:
                        # ç°¡å–®çš„checkboxå¯¦ç¾
                        checkbox = st.checkbox(
                            "",
                            value=is_selected,
                            key=f"doc_{doc['id']}"
                        )

                        # æ›´æ–°session state
                        if checkbox and doc['id'] not in st.session_state.selected_documents:
                            st.session_state.selected_documents.append(doc['id'])
                        elif not checkbox and doc['id'] in st.session_state.selected_documents:
                            st.session_state.selected_documents.remove(doc['id'])

                        if checkbox:
                            selected_count += 1

                    with col_info:
                        # é¡¯ç¤ºæ–‡ä»¶è³‡è¨Š
                        filename = Path(doc['source_path']).name if doc['source_path'] else "Unknown"
                        created_time = doc['created_at'].strftime("%Y-%m-%d %H:%M") if hasattr(doc['created_at'], 'strftime') else str(doc['created_at'])

                        st.markdown(f"""
                        **{doc['title']}**
                        ğŸ“„ æ–‡ä»¶å: {filename} | ğŸ“Š åˆ†å¡Šæ•¸: {doc['chunks']} | ğŸ•’ å‰µå»ºæ™‚é–“: {created_time[:16]}
                        ğŸ†” ID: `{doc['id'][:16]}...`
                        """)

                        # æ·»åŠ åˆ†éš”ç·š
                        st.divider()

            # é¡¯ç¤ºé¸æ“‡çµ±è¨ˆ
            if selected_count > 0:
                st.success(f"å·²é¸æ“‡ {selected_count} å€‹æ–‡ä»¶é€²è¡Œåˆªé™¤")
            else:
                st.info("æœªé¸æ“‡ä»»ä½•æ–‡ä»¶")

            # åˆªé™¤æŒ‰éˆ•å€åŸŸ
            if selected_count > 0:
                st.markdown("---")
                st.markdown("### ğŸ—‘ï¸ åŸ·è¡Œæ‰¹é‡åˆªé™¤")

                # ç¢ºèªæ–‡æœ¬
                confirm_text = f"ç¢ºèªåˆªé™¤é¸ä¸­çš„ {selected_count} å€‹æ–‡ä»¶å—ï¼Ÿæ­¤æ“ä½œä¸å¯é€†ï¼"

                # ä½¿ç”¨columnså‰µå»ºç¢ºèªå€åŸŸ
                col_confirm, col_button = st.columns([3, 1])

                with col_confirm:
                    # è¼¸å…¥ç¢ºèªæ–‡æœ¬ä¾†é˜²æ­¢æ„å¤–åˆªé™¤
                    user_confirmation = st.text_input(
                        "è«‹è¼¸å…¥ 'ç¢ºèªåˆªé™¤' ä»¥ç¹¼çºŒ",
                        placeholder=f"è¼¸å…¥ 'ç¢ºèªåˆªé™¤' ä¾†åˆªé™¤ {selected_count} å€‹æ–‡ä»¶",
                        help="é€™æ˜¯ç‚ºäº†é˜²æ­¢æ„å¤–åˆªé™¤ï¼Œè«‹ä»”ç´°ç¢ºèª"
                    )

                with col_button:
                    delete_enabled = user_confirmation == "ç¢ºèªåˆªé™¤"
                    delete_button = st.button(
                        f"ğŸ—‘ï¸ åˆªé™¤ {selected_count} å€‹æ–‡ä»¶",
                        type="primary",
                        disabled=not delete_enabled,
                        use_container_width=True
                    )

                if delete_button and delete_enabled:
                    # å°‡string IDsè½‰æ›ç‚ºUUID
                    selected_uuids = []
                    for doc_id_str in st.session_state.selected_documents:
                        try:
                            selected_uuids.append(UUID(doc_id_str))
                        except Exception as e:
                            st.error(f"ç„¡æ•ˆçš„Document ID: {doc_id_str}")
                            continue

                    if selected_uuids:
                        with st.spinner(f"æ­£åœ¨åˆªé™¤ {len(selected_uuids)} å€‹æ–‡æª”..."):
                            # åŸ·è¡Œæ‰¹é‡åˆªé™¤
                            results = asyncio.run(test_batch_document_deletion(selected_uuids))

                        # é¡¯ç¤ºè©³ç´°çµæœ
                        if results['successful_deletions'] > 0:
                            st.success(f"âœ… å®Œå…¨æˆåŠŸåˆªé™¤ {results['successful_deletions']} å€‹æ–‡ä»¶ï¼ˆNeo4j + Supabaseï¼‰")
                        else:
                            st.error("âŒ æ²’æœ‰æˆåŠŸåˆªé™¤ä»»ä½•æ–‡ä»¶")

                        # é¡¯ç¤ºå…·é«”å¤±æ•—æƒ…æ³
                        col_neo4j, col_supabase, col_partial = st.columns(3)

                        with col_neo4j:
                            if results.get('neo4j_failures'):
                                st.error(f"ğŸ—‚ï¸ Neo4j å¤±æ•—: {len(results['neo4j_failures'])} å€‹")
                            else:
                                st.success("ğŸ—‚ï¸ Neo4j: å…¨éƒ¨æˆåŠŸ")

                        with col_supabase:
                            if results.get('supabase_failures'):
                                st.warning(f"ğŸ—ƒï¸ Supabase å¤±æ•—: {len(results['supabase_failures'])} å€‹")
                            else:
                                st.success("ğŸ—ƒï¸ Supabase: å…¨éƒ¨æˆåŠŸ")

                        with col_partial:
                            if len(results['failed_deletions']) > results['successful_deletions']:
                                partial_failures = len(results['failed_deletions']) - (results.get('neo4j_failures', []) + results.get('supabase_failures', []))
                                if partial_failures > 0:
                                    st.warning(f"âš ï¸ éƒ¨åˆ†å¤±æ•—: {partial_failures} å€‹")
                            else:
                                st.success("ğŸ¯ åŒæ­¥åˆªé™¤: æˆåŠŸ")

                        # é¡¯ç¤ºå¤±æ•—çš„å…·é«”æ–‡æª”
                        if results['failed_deletions']:
                            st.warning(f"ğŸ”´ ä»¥ä¸‹æ–‡ä»¶åˆªé™¤å¤±æ•— ({len(results['failed_deletions'])} å€‹):")
                            failed_details = {
                                "Neo4j å¤±æ•—": results.get('neo4j_failures', []),
                                "Supabase å¤±æ•—": results.get('supabase_failures', [])
                            }

                            for failure_type, ids in failed_details.items():
                                if ids:
                                    st.markdown(f"**{failure_type}:**")
                                    for doc_id in ids:
                                        st.write(f"- `{doc_id[:16]}...`")
                                    st.markdown("")

                        # åªåœ¨æœ‰éŒ¯èª¤æ™‚é¡¯ç¤ºè©³ç´°éŒ¯èª¤ä¿¡æ¯
                        if results['errors']:
                            with st.expander("ğŸ” è©³ç´°éŒ¯èª¤ä¿¡æ¯", expanded=False):
                                for error in results['errors']:
                                    if "Neo4j failure" in error:
                                        st.error(f"ğŸ—‚ï¸ {error}")
                                    elif "Supabase failure" in error:
                                        st.warning(f"ğŸ—ƒï¸ {error}")
                                    else:
                                        st.code(error)

                        # å¦‚æœæœ‰æˆåŠŸåˆªé™¤ï¼Œé¡¯ç¤ºç‹€æ…‹åˆ†æä¸¦çµ¦ç”¨æˆ¶é¸æ“‡æ˜¯å¦åˆ·æ–°
                        if results['successful_deletions'] > 0:
                            # è¨­ç½®å…¨å±€åˆ·æ–°æ——æ¨™ï¼Œè¡¨ç¤ºæ•¸æ“šåº«å·²ç¶“ç™¼ç”Ÿè®ŠåŒ–
                            st.session_state.database_modified = True
                            st.session_state.last_refresh_time = time.time()

                            # é¡¯ç¤ºåˆªé™¤å¾Œç‹€æ…‹åˆ†æ
                            with st.expander("ğŸ“Š åˆªé™¤å¾Œç‹€æ…‹åˆ†æ", expanded=True):
                                display_post_deletion_status()

                            # çµ¦ç”¨æˆ¶é¸æ“‡æ˜¯å¦åˆ·æ–°é é¢
                            st.markdown("---")
                            col_refresh, col_keep = st.columns(2)

                            with col_refresh:
                                if st.button("ğŸ”„ åˆ·æ–°é é¢æŸ¥çœ‹æœ€æ–°ç‹€æ…‹", type="secondary", use_container_width=True):
                                    st.session_state.selected_documents = []  # æ¸…é™¤é¸æ“‡
                                    st.rerun()

                            with col_keep:
                                st.button("ğŸ“‹ ç¹¼çºŒæŸ¥çœ‹ç•¶å‰çµæœ", disabled=True, use_container_width=True)
                                st.info("ç•¶å‰é¡¯ç¤ºçš„æ˜¯åˆªé™¤æ“ä½œçš„å®Œæ•´çµæœï¼ŒåŒ…æ‹¬ Neo4j/Supabase çš„è©³ç´°ç‹€æ…‹")
                                st.info("ğŸ’¡ **æç¤º**: åˆ‡æ›åˆ°å…¶ä»–é é¢æ™‚ï¼Œç³»çµ±æœƒé¡¯ç¤ºæ•¸æ“šæ›´æ–°çš„è­¦å‘Šï¼Œå»ºè­°æ‰‹å‹•åˆ·æ–°æŸ¥çœ‹æœ€æ–°ç‹€æ…‹ã€‚")
                        else:
                            # æ²’æœ‰æˆåŠŸåˆªé™¤çš„æƒ…æ³ï¼Œçµ¦ç”¨æˆ·åˆ·æ–°é¸é …ä¾†é‡ç½®ä»‹é¢
                            if st.button("ğŸ”„ é‡æ–°è¼‰å…¥æ–‡ä»¶åˆ—è¡¨", type="secondary"):
                                st.session_state.selected_documents = []  # æ¸…é™¤é¸æ“‡
                                st.rerun()
        else:
            st.info("ğŸ“‚ ç›®å‰æ²’æœ‰å·²è™•ç†çš„æ–‡ä»¶å¯ä»¥åˆªé™¤")

    except Exception as e:
        st.error(f"è¼‰å…¥æ–‡ä»¶åˆ—è¡¨å¤±æ•—: {str(e)[:100]}...")
        st.code(str(e), language='text')

def show_chunk_deletion_test():
    """Chunkåˆªé™¤æ¸¬è©¦"""
    st.markdown("#### ğŸ“ Chunkåˆªé™¤æ¸¬è©¦")
    st.info("æ­¤åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­...")

def show_visual_fact_deletion_test():
    """Visual Factåˆªé™¤æ¸¬è©¦"""
    st.markdown("#### ğŸ‘ï¸ Visual Factåˆªé™¤æ¸¬è©¦")
    st.info("æ­¤åŠŸèƒ½æ­£åœ¨é–‹ç™¼ä¸­...")

async def test_document_deletion(document_id: str) -> bool:
    """æ¸¬è©¦Documentåˆªé™¤åŠŸèƒ½"""
    try:
        db_manager = DatabaseManager(
            neo4j_uri=settings.neo4j_uri,
            neo4j_user=settings.neo4j_user,
            neo4j_password=settings.neo4j_password,
            supabase_url=settings.supabase_url,
            supabase_key=settings.supabase_key
        )

        result = await db_manager.delete_document_cascade(document_id)
        await db_manager.close()
        return result

    except Exception as e:
        st.error(f"åˆªé™¤æ“ä½œå¤±æ•—: {str(e)}")
        return False

async def test_batch_document_deletion(document_ids: list) -> dict:
    """æ¸¬è©¦æ‰¹é‡Documentåˆªé™¤åŠŸèƒ½"""
    try:
        db_manager = DatabaseManager(
            neo4j_uri=settings.neo4j_uri,
            neo4j_user=settings.neo4j_user,
            neo4j_password=settings.neo4j_password,
            supabase_url=settings.supabase_url,
            supabase_key=settings.supabase_key
        )

        results = await db_manager.delete_documents_batch(document_ids)
        await db_manager.close()
        return results

    except Exception as e:
        error_msg = f"æ‰¹é‡åˆªé™¤æ“ä½œå¤±æ•—: {str(e)}"
        st.error(error_msg)
        return {
            "total_requested": len(document_ids),
            "successful_deletions": 0,
            "failed_deletions": [str(id) for id in document_ids],
            "errors": [error_msg]
        }

def display_post_deletion_status():
    """é¡¯ç¤ºåˆ é™¤æ“ä½œå¾Œçš„è³‡æ–™åº«ç‹€æ…‹åˆ†æ"""
    st.markdown("### ğŸ“ˆ åˆ é™¤å¾Œè³‡æ–™åº«ç‹€æ…‹")

    # æŸ¥è¯¢Neo4jç‹€æ…‹
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            settings.neo4j_uri,
            auth=(settings.neo4j_user, settings.neo4j_password)
        )

        with driver.session() as session:
            # Documentç¯€é»æ•¸é‡
            doc_result = session.run("MATCH (d:Document) RETURN count(d) as count")
            doc_count = doc_result.single()["count"]

            # Chunkç¯€é»æ•¸é‡
            chunk_result = session.run("MATCH (c:Chunk) RETURN count(c) as count")
            chunk_count = chunk_result.single()["count"]

            # Entityç¯€é»æ•¸é‡
            entity_result = session.run("MATCH (e:Entity) RETURN count(e) as count")
            entity_count = entity_result.single()["count"]

            # VisualFactç¯€é»æ•¸é‡
            vfact_result = session.run("MATCH (v:VisualFact) RETURN count(v) as count")
            vfact_count = vfact_result.single()["count"]

            # æ‰€æœ‰ç¯€é»æ•¸é‡
            all_result = session.run("MATCH (n) RETURN count(n) as count")
            all_nodes_count = all_result.single()["count"]

            # æ‰€æœ‰é—œä¿‚æ•¸é‡
            rel_result = session.run("MATCH ()-[r]-() RETURN count(DISTINCT r) as count")
            rel_count = rel_result.single()["count"]

        driver.close()

        # é¡¯ç¤ºNeo4jç‹€æ…‹
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ğŸ“„ Document ç¯€é»", doc_count)
            st.metric("ğŸ“ Chunk ç¯€é»", chunk_count)
        with col2:
            st.metric("ğŸ·ï¸ Entity ç¯€é»", entity_count)
            st.metric("ğŸ‘ï¸ VisualFact ç¯€é»", vfact_count)

        st.metric("ğŸ”— ç¸½ç¯€é»æ•¸", all_nodes_count)
        st.metric("âš¡ é—œä¿‚æ•¸", rel_count)

        if all_nodes_count == 0:
            st.success("âœ… Neo4jæ•¸æ“šåº«å·²è¢«æ¸…ç©º")
        elif doc_count == 0:
            st.info("â„¹ï¸ æ²’æœ‰Documentç¯€é»ï¼Œä½†ä»æœ‰å…¶ä»–è³‡æ–™")
        else:
            st.info(f"ğŸ“Š Neo4jä¸­å°šæœ‰ {doc_count} å€‹æ–‡æª”ç›¸é—œæ•¸æ“š")

    except Exception as e:
        st.error(f"âŒ ç„¡æ³•æŸ¥è©¢Neo4jç‹€æ…‹: {str(e)[:100]}...")

    st.markdown("---")

    # æŸ¥è¯¢Supabaseç‹€æ…‹
    try:
        from supabase import create_client
        client = create_client(settings.supabase_url, settings.supabase_key)

        # ç²å–vectorsè¡¨çµ±è¨ˆ
        response = client.table('vectors').select('*', count='exact').execute()
        vectors_count = response.count if hasattr(response, 'count') else len(response.data if response.data else [])

        # ä¸åŒå‘é‡é¡å‹çš„çµ±è¨ˆ
        if response.data:
            types = {}
            for item in response.data:
                vec_type = item.get('type', 'unknown')
                types[vec_type] = types.get(vec_type, 0) + 1

            type_counts = "\n".join([f"- **{t}**: {c} å€‹" for t, c in types.items()])
        else:
            type_counts = "ç„¡å‘é‡è³‡æ–™"

        # é¡¯ç¤ºSupabaseç‹€æ…‹
        st.metric("ğŸ—ƒï¸ Vector è¨˜éŒ„ç¸½æ•¸", vectors_count)

        if vectors_count > 0:
            st.markdown("**å‘é‡é¡å‹åˆ†ä½ˆ:**")
            st.markdown(type_counts)

            if vectors_count < 20:
                st.info("â„¹ï¸ å‘é‡è³‡æ–™é‡å¾ˆå°‘ï¼Œå¯èƒ½çš„å®Œå…¨æ¸…ç†")
            else:
                st.info(f"ğŸ“Š å‘é‡åŒ–è³‡æ–™åº«ä¸­å‰©é¤˜ {vectors_count} å€‹å‘é‡è¨˜éŒ„")
        else:
            st.success("âœ… Supabaseå‘é‡è³‡æ–™åº«å·²è¢«æ¸…ç©º")

    except Exception as e:
        st.error(f"âŒ ç„¡æ³•æŸ¥è©¢Supabaseç‹€æ…‹: {str(e)[:100]}...")

    # æ“ä½œæ‘˜è¦
    st.markdown("---")
    st.markdown("### ğŸ¯ åˆ é™¤æ“ä½œæ‘˜è¦")
    st.warning("âš ï¸ åˆ é™¤æ“ä½œå·²å®Œæˆã€‚æª¢æŸ¥ä¸Šæ–¹æ•¸æ“šåº«ç‹€æ…‹ä¾†ç¢ºèªæ¸…ç†æ•ˆæœã€‚")

# Page routing logic - moved to the end of file


def single_upload_handler(uploaded_file, vlm_strategy, force_vlm):
    """è™•ç†å–®å€‹æ–‡ä»¶ä¸Šå‚³"""
    if uploaded_file is not None:
        # æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨æ–¼è³‡æ–™åº«
        filename = uploaded_file.name
        file_extension = Path(filename).suffix.lower()

        # æª¢æŸ¥æ•¸æ“šåº«ä¸­æ˜¯å¦å­˜åœ¨
        if file_extension in ['.pdf', '.docx', '.txt', '.md']:
            file_exists_in_db, duplicate_file_info = check_file_exists(filename)

        # è™•ç†é‡å¤æ–‡ä»¶æƒ…å†µ
        should_proceed = handle_duplicate_file(file_exists_in_db, duplicate_file_info, filename)
        if not should_proceed:
            return

        # æ–‡ä»¶æ£€æŸ¥é€šè¿‡ï¼Œç»§ç»­æ­£å¸¸å¤„ç†
        show_file_info(uploaded_file)
        process_single_file(uploaded_file, vlm_strategy, force_vlm)
    else:
        st.info("è«‹ä¸Šå‚³ä¸€å€‹æ–‡ä»¶ä¾†é–‹å§‹æ¸¬è©¦")


def batch_upload_handler(uploaded_files, vlm_strategy, force_vlm):
    """è™•ç†æ‰¹é‡æ–‡ä»¶ä¸Šå‚³"""
    if uploaded_files:
        # é™åˆ¶æ–‡ä»¶æ•¸é‡
        max_files = 10
        if len(uploaded_files) > max_files:
            st.error(f"ğŸš« ä¸€æ¬¡æ€§æœ€å¤šåªèƒ½ä¸Šå‚³ {max_files} å€‹æ–‡ä»¶ã€‚æ‚¨ç›®å‰é¸æ“‡äº† {len(uploaded_files)} å€‹æ–‡ä»¶ã€‚")
            st.stop()

        st.success(f"ğŸ“‚ å·²é¸æ“‡ {len(uploaded_files)} å€‹æ–‡ä»¶")

        # æª¢æŸ¥æ¯å€‹æ–‡ä»¶çš„ç‹€æ…‹
        file_status = []
        for uploaded_file in uploaded_files:
            filename = uploaded_file.name
            file_extension = Path(filename).suffix.lower()

            exists, info = False, None
            if file_extension in ['.pdf', '.docx', '.txt', '.md']:
                exists, info = check_file_exists(filename)

            file_status.append({
                'file': uploaded_file,
                'filename': filename,
                'extension': file_extension,
                'exists': exists,
                'info': info
            })

        # åˆ†é¡é¡¯ç¤ºæ–‡ä»¶ç‹€æ…‹
        display_batch_file_status(file_status)

        # æ‰¹é‡è™•ç†æŒ‰éˆ•
        if all(not status['exists'] for status in file_status):
            # æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä»¥è™•ç†
            process_all_button = st.button("ğŸš€ æ‰¹é‡è™•ç†æ‰€æœ‰æ–‡ä»¶", type="primary", use_container_width=True)
            if process_all_button:
                process_batch_files(file_status, vlm_strategy, force_vlm)
        else:
            # æœ‰é‡å¤æ–‡ä»¶ï¼Œéœ€è¦ç”¨æˆ·é€‰æ‹©
            handle_batch_conflicts(file_status, vlm_strategy, force_vlm)
    else:
        st.info("è«‹é¸æ“‡å¤šå€‹æ–‡ä»¶é€²è¡Œæ‰¹é‡ä¸Šå‚³")


def check_file_exists(filename):
    """æª¢æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨æ–¼è³‡æ–™åº«"""
    file_exists_in_db = False
    duplicate_file_info = None

    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            settings.neo4j_uri,
            auth=(settings.neo4j_user, settings.neo4j_password)
        )

        with driver.session() as session:
            result = session.run("""
            MATCH (d:Document)
            WHERE d.source_path CONTAINS $filename
            RETURN d.document_id as id, d.title as title,
                   d.source_path as path, d.created_at as created_at
            ORDER BY d.created_at DESC
            LIMIT 1
            """, filename=filename)

            record = result.single()
            if record:
                file_exists_in_db = True
                duplicate_file_info = {
                    "id": record["id"],
                    "title": record["title"],
                    "path": record["path"],
                    "created_at": record["created_at"]
                }

        driver.close()

    except Exception as e:
        st.warning(f"âš ï¸ ç„¡æ³•æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å·²å­˜åœ¨: {str(e)[:50]}...")

    return file_exists_in_db, duplicate_file_info


def handle_duplicate_file(file_exists_in_db, duplicate_file_info, filename):
    """è™•ç†é‡è¤‡æ–‡ä»¶çš„æƒ…æ³"""
    if file_exists_in_db:
        st.error(f"ğŸš« **æª”æ¡ˆ '{filename}' å·²å­˜åœ¨æ–¼è³‡æ–™åº«ä¸­**")

        if duplicate_file_info:
            col1, col2 = st.columns(2)
            with col1:
                st.markdown(f"**æ¨™é¡Œ:** {duplicate_file_info['title']}")
                st.markdown(f"**è·¯å¾‘:** {Path(duplicate_file_info['path']).name}")
            with col2:
                created_time = duplicate_file_info['created_at'].strftime("%Y-%m-%d %H:%M") if hasattr(duplicate_file_info['created_at'], 'strftime') else str(duplicate_file_info['created_at'])
                st.markdown(f"**å‰µå»ºæ™‚é–“:** {created_time}")
                st.markdown(f"**æª”æ¡ˆID:** `{duplicate_file_info['id'][:16]}...`")

        # å¼ºåˆ¶ä¸Šä¼ é€‰é¡¹
        st.warning("âš ï¸ **è­¦å‘Š**: å¼·åˆ¶ä¸Šå‚³å°‡è¦†è“‹ç¾æœ‰æ•¸æ“šï¼Œå¯èƒ½å°è‡´æ•¸æ“šä¸ä¸€è‡´")

        col_skip, col_force = st.columns(2)
        with col_skip:
            if st.button("ğŸ”„ é‡æ–°é¸æ“‡æª”æ¡ˆ", use_container_width=True):
                st.stop()

        with col_force:
            if st.checkbox("âš¡ å¼·åˆ¶ä¸Šå‚³ä¸¦è¦†è“‹èˆŠç‰ˆæœ¬", key="force_upload_checkbox"):
                st.warning("âš ï¸ æ‚¨å·²å•Ÿç”¨å¼·åˆ¶ä¸Šå‚³æ¨¡å¼ã€‚è«‹è¬¹æ…ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚")
                st.info("ğŸ’¡ **å»ºè­°:** å¼·åˆ¶ä¸Šå‚³å¾Œè«‹æª¢æŸ¥æ•¸æ“šåº«å®Œæ•´æ€§ã€‚")
                return True  # å…è®¸ç»§ç»­
            else:
                st.info("ğŸ’¡ **å»ºè­°:** å…ˆåˆªé™¤èˆŠç‰ˆæœ¬æˆ–é‡æ–°å‘½åæª”æ¡ˆã€‚ç„¶è€Œï¼Œå¦‚æœéœ€è¦å¼·åˆ¶è¦†è“‹ï¼Œè«‹å‹¾é¸ä¸Šæ–¹é¸é …ã€‚")
                st.stop()
    return True


def show_file_info(uploaded_file):
    """é¡¯ç¤ºå–®å€‹æ–‡ä»¶çš„è³‡è¨Š"""
    filename = uploaded_file.name
    file_ext = Path(filename).suffix.lower()

    st.success(f"ğŸ“„ å·²é¸æ“‡: {filename}")

    file_info = {
        "æ–‡ä»¶å": filename,
        "å¤§å°": f"{uploaded_file.size/1024:.1f} KB",
        "æ ¼å¼": file_ext,
    }
    st.json(file_info)

    # VLMç­–ç•¥é©ç”¨æ€§æç¤º
    strategy_hint = {
        '.pdf': "å°‡ä½¿ç”¨VLMè™•ç†ï¼Œå› ç‚ºPDFéœ€è¦è¦–è¦ºåˆ†æ",
        '.docx': "å°‡å˜—è©¦VLMè™•ç†ï¼Œå¯å°è¤‡é›œæ ¼å¼çš„æ–‡ä»¶åˆ†æ",
        '.txt': "å°‡ç›´æ¥è™•ç†ï¼Œå› ç‚ºæ–‡å­—æ ¼å¼é©åˆLangChainè¼‰å…¥",
        '.md': "å°‡ç›´æ¥è™•ç†ï¼Œå› ç‚ºMarkdowné©åˆçµæ§‹åŒ–è§£æ"
    }

    if file_ext in strategy_hint:
        st.info(f"ğŸ¯ {strategy_hint[file_ext]}")


def process_single_file(uploaded_file, vlm_strategy, force_vlm):
    """è™•ç†å–®å€‹æ–‡ä»¶"""
    # è™•ç†æŒ‰éˆ•
    process_button = st.button("ğŸš€ é–‹å§‹è™•ç†", type="primary", use_container_width=True)

    if process_button:
        # æ–‡ä»¶è™•ç†é‚è¼¯ï¼ˆèˆ‡åŸæœ‰ç›¸åŒï¼‰
        with st.spinner("æ­£åœ¨è™•ç†æ–‡ä»¶..."):
            handle_file_processing(uploaded_file, vlm_strategy, force_vlm)


def display_batch_file_status(file_status):
    """é¡¯ç¤ºæ‰¹é‡ä¸Šå‚³æ–‡ä»¶çš„ç‹€æ…‹"""
    clean_files = [f for f in file_status if not f['exists']]
    duplicate_files = [f for f in file_status if f['exists']]

    st.markdown("### ğŸ“Š æ–‡ä»¶ç‹€æ…‹çµ±è¨ˆ")
    col1, col2 = st.columns(2)

    with col1:
        st.metric("âœ… å¯è™•ç†æ–‡ä»¶", len(clean_files))

    with col2:
        st.metric("ğŸš« é‡è¤‡æ–‡ä»¶", len(duplicate_files))

    # é¡¯ç¤ºå¯è™•ç†çš„æ–‡ä»¶
    if clean_files:
        st.markdown("#### âœ… å¯è™•ç†çš„æ–‡ä»¶")
        for file_info in clean_files:
            st.markdown(f"ğŸ“„ {file_info['filename']} ({file_info['extension']})")

    # é¡¯ç¤ºé‡è¤‡çš„æ–‡ä»¶
    if duplicate_files:
        st.markdown("#### ğŸš« é‡è¤‡çš„æ–‡ä»¶")
        for file_info in duplicate_files:
            with st.expander(f"ğŸ“„ {file_info['filename']} - å·²å­˜åœ¨", expanded=False):
                if file_info['info']:
                    st.markdown(f"**ç¾æœ‰æ–‡ä»¶è³‡è¨Š:**")
                    info_col1, info_col2 = st.columns(2)
                    with info_col1:
                        st.markdown(f"æ¨™é¡Œ: {file_info['info']['title']}")
                        st.markdown(f"è·¯å¾‘: {Path(file_info['info']['path']).name}")
                    with info_col2:
                        created_time = file_info['info']['created_at'].strftime("%Y-%m-%d %H:%M") if hasattr(file_info['info']['created_at'], 'strftime') else str(file_info['info']['created_at'])
                        st.markdown(f"å‰µå»ºæ™‚é–“: {created_time}")
                        st.markdown(f"æª”æ¡ˆID: `{file_info['info']['id'][:16]}...`")


def handle_batch_conflicts(file_status, vlm_strategy, force_vlm):
    """è™•ç†æ‰¹é‡ä¸Šå‚³ä¸­çš„è¡çª"""
    clean_files = [f for f in file_status if not f['exists']]
    duplicate_files = [f for f in file_status if f['exists']]

    if clean_files:
        st.info(f"ğŸ’¡ {len(clean_files)} å€‹æ–‡ä»¶å¯ä»¥ç›´æ¥è™•ç†ï¼Œ{len(duplicate_files)} å€‹æ–‡ä»¶éœ€è¦è™•ç†é‡è¤‡å•é¡Œã€‚")

        # è™•ç†å¯ç›´æ¥è™•ç†çš„æ–‡ä»¶
        process_clean_button = st.button(f"ğŸš€ è™•ç†å¯ç›´æ¥è™•ç†çš„æ–‡ä»¶ ({len(clean_files)} å€‹)",
                                       type="primary", use_container_width=True)
        if process_clean_button:
            process_batch_files(clean_files, vlm_strategy, force_vlm)

    # å¤„ç†é‡å¤æ–‡ä»¶çš„é€‰é¡¹
    if duplicate_files:
        st.markdown("### ğŸ”„ è™•ç†é‡è¤‡æ–‡ä»¶")
        st.warning("å°æ–¼é‡è¤‡çš„æ–‡ä»¶ï¼Œæ‚¨å¯ä»¥é¸æ“‡ï¼š")

        col1, col2 = st.columns(2)

        with col1:
            skip_duplicates_button = st.button("â­ï¸ è·³éé‡è¤‡æ–‡ä»¶",
                                             help="åªè™•ç†æœªé‡è¤‡çš„æ–‡ä»¶",
                                             use_container_width=True)

        with col2:
            force_all_button = st.button("âš¡ å¼·åˆ¶è™•ç†æ‰€æœ‰æ–‡ä»¶",
                                       help="è¦†è“‹æ‰€æœ‰é‡è¤‡æ–‡ä»¶ï¼Œè«‹è¬¹æ…ä½¿ç”¨",
                                       type="secondary",
                                       use_container_width=True)

        if skip_duplicates_button:
            process_batch_files(clean_files, vlm_strategy, force_vlm)

        if force_all_button:
            # å¼ºåˆ¶å¤„ç†æ‰€æœ‰æ–‡ä»¶
            st.warning("âš¡ å·²å•Ÿç”¨å¼·åˆ¶è¦†è“‹æ¨¡å¼ï¼Œæ­£åœ¨è™•ç†æ‰€æœ‰æ–‡ä»¶...")
            process_batch_files(file_status, vlm_strategy, force_vlm, force_override=True)


async def process_batch_files(files, vlm_strategy, force_vlm, force_override=False):
    """æ‰¹é‡è™•ç†æ–‡ä»¶"""
    if not files:
        st.warning("æ²’æœ‰æ–‡ä»¶å¯ä»¥è™•ç†")
        return

    # å‰µå»ºé€²åº¦é¡¯ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    result_container = st.empty()

    total_files = len(files)
    successful_uploads = 0
    failed_uploads = 0
    failed_details = []

    ingestion_service = IngestionService()

    for i, file_info in enumerate(files):
        current_file = file_info['file']
        filename = file_info['filename']

        status_text.text(f"è™•ç†æ–‡ä»¶ {i+1}/{total_files}: {filename}")

        try:
            # ä¿å­˜æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
            with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{current_file.name}") as tmp_file:
                tmp_file.write(current_file.read())
                file_path = Path(tmp_file.name)

            progress_bar.progress((i) / total_files, f"è™•ç†ä¸­: {filename}")

            # è™•ç†æ–‡ä»¶
            result = await ingestion_service.ingest_document_enhanced(
                file_path=file_path,
                force_vlm=force_vlm
            )

            if result.get("success"):
                successful_uploads += 1
                st.success(f"âœ… {filename} è™•ç†æˆåŠŸ")
            else:
                failed_uploads += 1
                failed_details.append({
                    'filename': filename,
                    'error': result.get('error', 'æœªçŸ¥éŒ¯èª¤')
                })
                st.error(f"âŒ {filename} è™•ç†å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

            # æ¸…ç†è‡¨æ™‚æ–‡ä»¶
            try:
                file_path.unlink()
            except:
                pass

        except Exception as e:
            failed_uploads += 1
            failed_details.append({
                'filename': filename,
                'error': str(e)
            })
            st.error(f"âŒ {filename} è™•ç†ç•°å¸¸: {str(e)}")

    progress_bar.progress(1.0, "æ‰¹é‡è™•ç†å®Œæˆ")
    status_text.empty()
    progress_bar.empty()

    # é¡¯ç¤ºæ‰¹é‡è™•ç†çµæœçµ±è¨ˆ
    with result_container.container():
        st.markdown("### ğŸ“ˆ æ‰¹é‡è™•ç†çµæœçµ±è¨ˆ")

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("ğŸ“‚ ç¸½æ–‡ä»¶æ•¸", total_files)

        with col2:
            st.metric("âœ… æˆåŠŸè™•ç†", successful_uploads)

        with col3:
            st.metric("âŒ è™•ç†å¤±æ•—", failed_uploads)

        if failed_details:
            st.markdown("### âŒ å¤±æ•—è©³æƒ…")
            for failure in failed_details:
                with st.expander(f"âŒ {failure['filename']}", expanded=False):
                    st.code(failure['error'])

        if successful_uploads > 0:
            st.success(f"ğŸ‰ æ‰¹é‡ä¸Šå‚³å®Œæˆï¼æˆåŠŸè™•ç† {successful_uploads} å€‹æ–‡ä»¶ã€‚")

            # æ˜¾ç¤ºæ•°æ®æ›´æ–°æ‘˜è¦
            st.markdown("### ğŸ“Š æ•°æ®æ›´æ–°æ‘˜è¦")
            display_post_processing_status()


def handle_file_processing(uploaded_file, vlm_strategy, force_vlm):
    """è™•ç†å–®å€‹æ–‡ä»¶çš„å®Œæ•´é‚è¼¯"""
    # å‰µå»ºé€²åº¦æ¢
    progress_bar = st.progress(0, "åˆå§‹åŒ–...")
    status_text = st.empty()
    result_area = st.empty()

    try:
        # ä¿å­˜ä¸Šå‚³æ–‡ä»¶åˆ°è‡¨æ™‚ä½ç½®
        with tempfile.NamedTemporaryFile(delete=False, suffix=f"_{uploaded_file.name}") as tmp_file:
            tmp_file.write(uploaded_file.read())
            file_path = Path(tmp_file.name)

        progress_bar.progress(10, "æ–‡ä»¶è¼‰å…¥ä¸­...")

        # åˆå§‹åŒ–è™•ç†æœå‹™
        status_text.text("ğŸ”§ åˆå§‹åŒ–è™•ç†æœå‹™...")
        progress_bar.progress(20, "åˆå§‹åŒ–æœå‹™...")

        ingestion_service = IngestionService()

        progress_bar.progress(30, "é–‹å§‹è™•ç†...")

        # åŸ·è¡Œå¢å¼·è™•ç†
        status_text.text(f"ğŸ¯ è™•ç†ä¸­ ({vlm_strategy})...")

        start_time = time.time()

        result = asyncio.run(ingestion_service.ingest_document_enhanced(
            file_path=file_path,
            force_vlm=force_vlm
        ))

        processing_time = time.time() - start_time

        progress_bar.progress(100, "è™•ç†å®Œæˆ! ğŸ‰")

        # é¡¯ç¤ºçµæœ
        with result_area.container():
            if result.get("success"):
                st.success("ğŸ‰ è™•ç†æˆåŠŸï¼")
                st.metric("è™•ç†æ™‚é–“", f"{processing_time:.1f}s")
                st.metric("åˆ†å¡Šæ•¸", result.get("metadata", {}).get("chunks_created", 0))
                st.metric("å‘é‡æ•¸", result.get("metadata", {}).get("embeddings_created", 0))

                # é¡¯ç¤ºè©³ç´°è³‡è¨Š
                with st.expander("ğŸ“‹ è™•ç†è©³æƒ…", expanded=True):
                    processing_trace = result.get("processing_trace", {})
                    if processing_trace:
                        st.write(f"**æ–‡ä»¶é¡å‹**: {processing_trace.get('file_type', 'Unknown')}")
                        st.write(f"**ä½¿ç”¨æ¨¡çµ„**: {', '.join(processing_trace.get('modules_used', []))}")

            else:
                st.error(f"âŒ è™•ç†å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")

    except Exception as e:
        st.error(f"âŒ è™•ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        with result_area.container():
            with st.expander("éŒ¯èª¤è©³æƒ…", expanded=True):
                st.code(str(e), language='text')

    finally:
        # æ¸…ç†é€²åº¦é¡¯ç¤º
        status_text.empty()
        progress_bar.empty()


def display_post_processing_status():
    """é¡¯ç¤ºè™•ç†å¾Œçš„è³‡æ–™åº«ç‹€æ…‹"""
    try:
        from neo4j import GraphDatabase
        driver = GraphDatabase.driver(
            settings.neo4j_uri,
            auth=(settings.neo4j_user, settings.neo4j_password)
        )

        with driver.session() as session:
            doc_result = session.run("MATCH (d:Document) RETURN count(d) as count")
            doc_count = doc_result.single()["count"]

        driver.close()

        # æ˜¾ç¤ºSupabaseçµ±è¨ˆ
        from supabase import create_client
        client = create_client(settings.supabase_url, settings.supabase_key)
        vectors_response = client.table('vectors').select('*', count='exact').execute()
        vectors_count = vectors_response.count if hasattr(vectors_response, 'count') else 0

        st.metric("ğŸ—‚ï¸ Neo4j æ–‡ä»¶æ•¸", doc_count)
        st.metric("ğŸ—ƒï¸ Supabase å‘é‡æ•¸", vectors_count)

    except Exception as e:
        st.warning(f"ç„¡æ³•æª¢æŸ¥æœ€æ–°çµ±è¨ˆ: {str(e)[:50]}...")


# é é¢è·¯ç”±ï¼šåªä¿ç•™ä¸€å€‹é é¢è·¯ç”±
if page == "æ–‡æª”è™•ç†":
    st.markdown("# ğŸ“„ æ–‡æª”è™•ç†")

    # å´é‚Šæ¬„çš„è™•ç†é…ç½®é‚è¼¯
    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown("### ğŸ“¤ æ–‡ä»¶ä¸Šå‚³æ¨¡å¼")

        # é¸æ“‡ä¸Šå‚³æ¨¡å¼
        upload_mode = st.radio(
            "é¸æ“‡ä¸Šå‚³é¡å‹",
            ["å–®å€‹æ–‡ä»¶", "æ‰¹é‡ä¸Šå‚³"],
            index=0,
            key="upload_mode_radio",
            help="""
            å–®å€‹æ–‡ä»¶: ä¸Šå‚³ä¸€å€‹æ–‡ä»¶é€²è¡Œè™•ç†
            æ‰¹é‡ä¸Šå‚³: ä¸Šå‚³å¤šå€‹æ–‡ä»¶åŒæ™‚è™•ç†
            """
        )

        st.markdown("---")

        if upload_mode == "å–®å€‹æ–‡ä»¶":
            # å–®æ–‡ä»¶ä¸Šå‚³ (åŸæœ‰é‚è¼¯)
            st.markdown("### ğŸ“„ å–®å€‹æ–‡ä»¶ä¸Šå‚³")

            # æ–‡ä»¶ä¸Šå‚³å™¨
            uploaded_file = st.file_uploader(
                "é¸æ“‡æ¸¬è©¦æ–‡æª”",
                type=["pdf", "docx", "txt", "md"],
                help="æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: PDF, Word, æ–‡å­—, Markdown",
                key="uploaded_file"
            )

            # å°‡å¾ŒçºŒé‚è¼¯åŒ…è£åœ¨è®Šæ•¸ä¸­ï¼Œä»¥ä¾¿é‡ç”¨
            single_upload_handler(uploaded_file, vlm_strategy, force_vlm)

        else:  # æ‰¹é‡ä¸Šå‚³
            # æ‰¹é‡ä¸Šå‚³é‚è¼¯
            st.markdown("### ğŸ“‚ æ‰¹é‡æ–‡ä»¶ä¸Šå‚³")

            # æ‰¹é‡æ–‡ä»¶ä¸Šå‚³å™¨
            uploaded_files = st.file_uploader(
                "é¸æ“‡å¤šå€‹æ¸¬è©¦æ–‡æª”",
                type=["pdf", "docx", "txt", "md"],
                accept_multiple_files=True,
                help="æ”¯æ´çš„æ–‡ä»¶æ ¼å¼: PDF, Word, æ–‡å­—, Markdownï¼Œä¸€æ¬¡æœ€å¤šé¸æ“‡10å€‹æ–‡ä»¶",
                key="uploaded_files"
            )

            # è™•ç†æ‰¹é‡ä¸Šå‚³
            batch_upload_handler(uploaded_files, vlm_strategy, force_vlm)

    with col2:
        st.markdown("### ğŸ“Š è™•ç†çµæœ")

        # å¤„ç†ç»“æœæ˜¾ç¤ºåŒºåŸŸä¼šåœ¨å„å¤„ç†å‡½æ•°ä¸­æ˜¾ç¤º

        # æ–‡æª”è™•ç†é é¢åº•éƒ¨ä¿¡æ¯
        st.markdown("---")
        st.markdown("""
### ğŸ“– ä½¿ç”¨èªªæ˜

#### ğŸš€ **æ–°å¢åŠŸèƒ½: æ‰¹é‡æ–‡ä»¶ä¸Šå‚³**

**ç‰¹è‰²åŠŸèƒ½:**
- ğŸ“‚ **æ‰¹é‡é¸æ“‡**: ä¸€æ¬¡é¸æ“‡æœ€å¤š10å€‹æ–‡ä»¶
- ğŸ” **æ™ºèƒ½æª¢æŸ¥**: è‡ªå‹•æª¢æŸ¥æ¯å€‹æ–‡ä»¶æ˜¯å¦é‡è¤‡
- âš¡ **æ‰¹é‡è™•ç†**: åŒæ™‚è™•ç†å¤šå€‹æ–‡ä»¶
- ğŸ“Š **è©³ç´°çµ±è¨ˆ**: æä¾›å®Œæ•´çš„æˆåŠŸ/å¤±æ•—çµ±è¨ˆ
- ğŸ›¡ï¸ **è¡çªè§£æ±º**: è™•ç†é‡è¤‡æ–‡ä»¶æ™‚æä¾›å¤šç¨®é¸æ“‡

**ä½¿ç”¨æ­¥é©Ÿ:**
1. **é¸æ“‡æ¨¡å¼**: å¾"å–®å€‹æ–‡ä»¶"æˆ–"æ‰¹é‡ä¸Šå‚³"ä¸­é¸æ“‡
2. **é¸æ“‡æ–‡ä»¶**: å°æ–¼æ‰¹é‡ä¸Šå‚³ï¼Œé¸æ“‡å¤šå€‹æ–‡ä»¶
3. **æª¢æŸ¥ç‹€æ…‹**: ç³»çµ±è‡ªå‹•åˆ†ææ–‡ä»¶ç‹€æ…‹
4. **è™•ç†æ–‡ä»¶**: æ ¹æ“šç‹€æ…‹é¸æ“‡è™•ç†ç­–ç•¥
5. **æŸ¥çœ‹çµæœ**: ç²å–å®Œæ•´çš„è™•ç†çµ±è¨ˆè³‡è¨Š

#### ğŸ“„ **æ—¢æœ‰åŠŸèƒ½: å–®å€‹æ–‡ä»¶ä¸Šå‚³**
- ğŸš€ **å¿«é€Ÿè™•ç†**: ä¸Šå‚³ä¸€å€‹æ–‡ä»¶å³æ™‚è™•ç†
- ğŸ›¡ï¸ **é‡è¤‡æª¢æŸ¥**: é¿å…æ•¸æ“šé‡è¤‡
- âš¡ **è¦†è“‹é¸é …**: æä¾›å¼·åˆ¶è¦†è“‹åŠŸèƒ½
- ğŸ“‹ **è©³ç´°å›å ±**: å®Œæ•´çš„è™•ç†è»Œè·¡å’Œçµ±è¨ˆ

### ğŸ¯ **ç³»çµ±ç‰¹è‰²**

#### ğŸ”„ **è™•ç†å„ªå…ˆé †åº**
1. **VLMæœå‹™** (å¦‚æœé‹è¡Œ) â†’ Ollamaæˆ–OpenAI
2. **MinerU** â†’ å¦‚æœVLMå¤±æ•—æˆ–è·³é
3. **Tesseract OCR** â†’ æœ€çµ‚é™ç´šé¸é …
4. **æ–‡å­—è™•ç†** â†’ å°æ–¼.txt/.mdæ–‡ä»¶

#### ğŸ¨ **æ™ºèƒ½ç­–ç•¥**
- **è‡ªå‹•åˆ¤æ–·**: æ ¹æ“šæ–‡ä»¶é¡å‹æ™ºèƒ½é¸æ“‡è™•ç†å™¨
- **å¼·åˆ¶é–‹å•Ÿ**: é‡å°æ‰€æœ‰æ–‡ä»¶å˜—è©¦VLMè™•ç†
- **å¼·åˆ¶é—œé–‰**: ç›´æ¥ä½¿ç”¨LangChainè¼‰å…¥

#### ğŸ“Š **å“è³ªè¿½è¹¤**
- **è™•ç†æ™‚é–“ç›£æ§**: å„éšæ®µæ™‚é–“çµ±è¨ˆ
- **æˆåŠŸç‡åˆ†æ**: æ–‡ä»¶è™•ç†æˆåŠŸ/å¤±æ•—çµ±è¨ˆ
- **éŒ¯èª¤åˆ†é¡**: è©³ç´°çš„éŒ¯èª¤é¡å‹å’ŒåŸå› 

---
**ğŸ‰ æ–°åŠŸèƒ½ä¸Šç·š: æ‰¹é‡æ–‡ä»¶è™•ç†ï¼Œè®“æ•¸æ“šå°å…¥æ›´é«˜æ•ˆï¼**
        """)

# è³‡æ–™åº«ç®¡ç†é é¢
elif page == "è³‡æ–™åº«ç®¡ç†":
    show_database_management_page()

# ğŸ”§ é…ç½®ç³»çµ±æ¶æ§‹èªªæ˜ (åŒ…å«LLMé…ç½®)

## ğŸ“‹ é…ç½®å±¤æ¬¡çµæ§‹

```
ç’°å¢ƒè®Šæ•¸ (.env) â† æœ€é«˜å„ªå…ˆç´š (é‹è¡Œæ™‚è¦†è“‹)
    â†“
è¨­å®šé¡ (config.py) â† ä¸­é–“å±¤ (å¸¶é è¨­å€¼)
    â†“
å¸¸æ•¸ (constants.py) â† åŸºç¤å±¤ (æ‡‰ç”¨é‚è¼¯å¸¸æ•¸)
```

## ğŸ¤– LLM é…ç½®ç³»çµ±

### æ¶æ§‹è¨­è¨ˆ

å°ˆæ¡ˆå¯¦ç¾äº†**é›†ä¸­å¼LLMé…ç½®ç®¡ç†ç³»çµ±**ï¼Œæ”¯æŒå¤šç¨®LLMæä¾›å•†å’Œå‹•æ…‹é…ç½®ï¼š

```
ç’°å¢ƒè®Šæ•¸ (.env)
    â†“
Settings (config.py)
    â†“
LLMFactory (llm_factory.py)
    â†“
å„Agentå¯¦ä¾‹
```

## ğŸ¯ å„å±¤ç´šèªªæ˜

### 1. ç’°å¢ƒè®Šæ•¸ (.env) - æœ€é«˜å„ªå…ˆç´š

#### ç”¨é€”
- **ç’°å¢ƒç‰¹å®šé…ç½®**: é–‹ç™¼/æ¸¬è©¦/ç”Ÿç”¢ç’°å¢ƒå·®ç•°
- **æ•æ„Ÿè³‡è¨Š**: API é‡‘é‘°ã€è³‡æ–™åº«å¯†ç¢¼
- **å‹•æ…‹èª¿æ•´**: é‹è¡Œæ™‚è¦†è“‹è€Œä¸ä¿®æ”¹ä»£ç¢¼
- **å®‰å…¨æ€§**: ä¸æ‡‰æäº¤åˆ°ç‰ˆæœ¬æ§åˆ¶

#### ç¯„ä¾‹ `.env` æ–‡ä»¶
```bash
# è³‡æ–™åº«é…ç½®
NEO4J_URI=neo4j://production-server:7687
NEO4J_USER=myuser
NEO4J_PASSWORD=mysecret

# API é‡‘é‘°
OPENAI_API_KEY=sk-your-openai-key-here
SUPABASE_KEY=your-supabase-anon-key

# LLM é…ç½®
LLM_PROVIDER=openai
LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.1
PLANNER_LLM_MODEL=gpt-4o-mini
ANSWERER_LLM_MODEL=gpt-4

# æ‡‰ç”¨é…ç½®
DEBUG=false
LOG_LEVEL=WARNING
KNOWLEDGE_AREA_ID=production_area

# è™•ç†åƒæ•¸ (å¯è¦†è“‹é è¨­å€¼)
CHUNK_SIZE=1500
MIN_ENTITY_CONFIDENCE=0.7
```

#### è®€å–æ–¹å¼
- ç³»çµ±ç’°å¢ƒè®Šæ•¸
- `.env` æ–‡ä»¶ (ç”± python-dotenv è¼‰å…¥)
- Docker/Kubernetes ç’°å¢ƒè®Šæ•¸

### 2. è¨­å®šé¡ (config.py) - ä¸­é–“å±¤

#### ç”¨é€”
- **é¡å‹å®‰å…¨**: ä½¿ç”¨ Pydantic é€²è¡Œé¡å‹é©—è­‰
- **é è¨­å€¼ç®¡ç†**: æä¾›åˆç†çš„é è¨­é…ç½®
- **ç’°å¢ƒè®Šæ•¸æ•´åˆ**: è‡ªå‹•å¾ç’°å¢ƒè®Šæ•¸è®€å–
- **é©—è­‰èˆ‡è½‰æ›**: è³‡æ–™é¡å‹è½‰æ›å’Œé©—è­‰

#### å¯¦ç¾æ–¹å¼

```python
# grag/core/config.py
from pydantic_settings import BaseSettings
from .constants import DEFAULT_CHUNK_SIZE, VECTOR_DIMENSIONS

class Settings(BaseSettings):
    # è³‡æ–™åº«é…ç½® (å¯è¢« .env è¦†è“‹)
    neo4j_uri: str = "neo4j://localhost:7687"
    neo4j_user: str = "neo4j"

    # è™•ç†åƒæ•¸ (ä½¿ç”¨å¸¸æ•¸ä½œç‚ºé è¨­å€¼)
    chunk_size: int = DEFAULT_CHUNK_SIZE
    embedding_dimension: int = VECTOR_DIMENSIONS["sentence_transformers"]

    class Config:
        env_file = ".env"
        case_sensitive = False
```

#### ç‰¹é»
- âœ… **è‡ªå‹•ç’°å¢ƒè®Šæ•¸æ˜ å°„**: `neo4j_uri` â†’ `NEO4J_URI`
- âœ… **é è¨­å€¼æ”¯æ´**: ç’°å¢ƒè®Šæ•¸ä¸å­˜åœ¨æ™‚ä½¿ç”¨é è¨­å€¼
- âœ… **é¡å‹é©—è­‰**: ç¢ºä¿é…ç½®å€¼çš„æ­£ç¢ºé¡å‹
- âœ… **å¿«å–å¯¦ä¾‹**: ä½¿ç”¨ `@lru_cache` é¿å…é‡è¤‡åˆå§‹åŒ–

#### LLM é…ç½®å¯¦ç¾

```python
class Settings(BaseSettings):
    # === AI Model Configuration ===
    # LLM Configuration (Centralized)
    llm_provider: str = "openai"  # openai, ollama, vllm, lmstudio, custom, etc.
    llm_model: str = "gpt-4o-mini"
    llm_temperature: float = 0.1
    llm_max_tokens: int = 2000
    openai_api_key: str = ""  # Will be read from OPENAI_API_KEY env var

    # Agent-specific LLM configurations
    planner_llm_model: str = "gpt-4o-mini"  # Query planning - needs precision
    reasoner_llm_model: str = "gpt-4o-mini"  # Reasoning tasks - needs analysis
    answerer_llm_model: str = "gpt-4"       # Final answer generation - needs quality
    query_parser_llm_model: str = "gpt-4o"  # Structured query parsing - needs understanding

    # Agent-specific temperature settings
    query_parser_temperature: float = 0.1   # Low temperature for consistent parsing
    answerer_temperature: float = 0.3       # Slightly higher for natural responses

    # Ollama (local VLM service)
    ollama_base_url: str = "http://localhost:11434/v1"
    ollama_api_key: str = "ollama"
    ollama_model: str = "qwen3-vl:235b-cloud"

### 3. å¸¸æ•¸ (constants.py) - åŸºç¤å±¤

#### ç”¨é€”
- **æ‡‰ç”¨é‚è¼¯å¸¸æ•¸**: ä¸æ‡‰è®ŠåŒ–çš„å›ºå®šå€¼
- **æšèˆ‰èˆ‡é¸é …**: æ”¯æ´çš„æ ¼å¼ã€å“è³ªç­‰ç´š
- **é è¨­åƒæ•¸**: è™•ç†æ¼”ç®—æ³•çš„åŸºç¤åƒæ•¸
- **éŒ¯èª¤è¨Šæ¯**: çµ±ä¸€çš„éŒ¯èª¤è¨Šæ¯å®šç¾©

#### ç¯„ä¾‹å…§å®¹

```python
# grag/core/constants.py

# è™•ç†åƒæ•¸
DEFAULT_CHUNK_SIZE = 1000
OVERLAP_SIZE = 200
DEFAULT_ENTITY_CONFIDENCE = 0.5

# æ”¯æ´æ ¼å¼
SUPPORTED_EXTENSIONS = {
    "pdf": "pdf",
    "word": "docx",
    "images": ["png", "jpg", "jpeg"]
}

# å“è³ªç­‰ç´š
QUALITY_LEVELS = ["high", "medium", "low", "unknown"]

# å‘é‡ç¶­åº¦æ˜ å°„
VECTOR_DIMENSIONS = {
    "sentence_transformers": 384,
    "openai": 1536,
    "cohere": 1024
}
```

#### ç‰¹é»
- âœ… **ç‰ˆæœ¬æ§åˆ¶**: éš¨ä»£ç¢¼ä¸€èµ·ç®¡ç†
- âœ… **æ–‡æª”åŒ–**: æ¸…æ¥šèªªæ˜æ¯å€‹å¸¸æ•¸ç”¨é€”
- âœ… **å¯å¼•ç”¨**: å…¶ä»–æ¨¡çµ„å¯ç›´æ¥ import ä½¿ç”¨
- âœ… **æ¸¬è©¦å‹å¥½**: å¸¸æ•¸ä¾¿æ–¼å–®å…ƒæ¸¬è©¦

## ğŸ”„ é…ç½®è§£ææµç¨‹

### å®Œæ•´è§£æé †åº

1. **ç’°å¢ƒè®Šæ•¸æª¢æŸ¥**
   ```bash
   # æª¢æŸ¥ç³»çµ±ç’°å¢ƒè®Šæ•¸
   echo $NEO4J_URI

   # æª¢æŸ¥ .env æ–‡ä»¶
   cat .env | grep NEO4J_URI
   ```

2. **Pydantic Settings è™•ç†**
   ```python
   # è‡ªå‹•è½‰æ›ç’°å¢ƒè®Šæ•¸åç¨±
   neo4j_uri â†’ NEO4J_URI (å¤§å¯«)
   chunk_size â†’ CHUNK_SIZE

   # ä½¿ç”¨é è¨­å€¼ (å¦‚æœç’°å¢ƒè®Šæ•¸ä¸å­˜åœ¨)
   neo4j_uri = os.getenv('NEO4J_URI', 'neo4j://localhost:7687')
   ```

3. **å¸¸æ•¸æ•´åˆ**
   ```python
   # config.py ä½¿ç”¨ constants.py çš„å€¼
   from .constants import DEFAULT_CHUNK_SIZE
   chunk_size: int = DEFAULT_CHUNK_SIZE
   ```

### å¯¦éš›ä½¿ç”¨ç¯„ä¾‹

```python
from grag.core.config import settings

# é€™äº›å€¼å¯èƒ½ä¾†è‡ªï¼š
# 1. ç’°å¢ƒè®Šæ•¸ NEO4J_URI
# 2. .env æ–‡ä»¶ä¸­çš„ neo4j_uri
# 3. é è¨­å€¼ "neo4j://localhost:7687"
print(f"Neo4j URI: {settings.neo4j_uri}")

# é€™äº›å€¼ä¾†è‡ª constants.pyï¼š
# DEFAULT_CHUNK_SIZE = 1000
print(f"Chunk size: {settings.chunk_size}")

# å¦‚æœç’°å¢ƒè®Šæ•¸å­˜åœ¨ï¼Œæœƒè¦†è“‹å¸¸æ•¸çš„é è¨­å€¼
# CHUNK_SIZE=1500 â†’ settings.chunk_size = 1500
```

## ğŸ› ï¸ é…ç½®ç®¡ç†æœ€ä½³å¯¦è¸

### ç’°å¢ƒè®Šæ•¸å‘½åæ…£ä¾‹
```bash
# è³‡æ–™åº«ç›¸é—œ
NEO4J_URI=...
SUPABASE_URL=...

# AI æ¨¡å‹ç›¸é—œ
OPENAI_API_KEY=...
OLLAMA_BASE_URL=...

# æ‡‰ç”¨é…ç½®
DEBUG=true
LOG_LEVEL=INFO
```

### å¸¸æ•¸å®šç¾©åŸå‰‡
```python
# âœ… å¥½çš„å¸¸æ•¸å®šç¾©
DEFAULT_TIMEOUT = 120  # ç§’
MAX_RETRIES = 3
SUPPORTED_FORMATS = ["pdf", "docx", "txt"]

# âŒ é¿å…çš„åšæ³•
timeout = 120  # æ²’æœ‰èªªæ˜å–®ä½
formats = ["pdf", "docx"]  # æ²’æœ‰ DEFAULT_ å‰ç¶´
```

### è¨­å®šé¡çµ„ç¹”
```python
class Settings(BaseSettings):
    # æŒ‰åŠŸèƒ½åˆ†çµ„ä¸¦åŠ ä¸Šè¨»é‡‹
    # === Database Configuration ===
    neo4j_uri: str = "..."

    # === AI Model Configuration ===
    llm_model: str = "..."

    # === Processing Configuration ===
    chunk_size: int = DEFAULT_CHUNK_SIZE
```

## ğŸ¤– Agent å°ˆç”¨ LLM é…ç½®

### å„ Agent çš„æœ€ä½³é…ç½®

#### **1. Query Planner (æŸ¥è©¢è¦åŠƒ)**
```python
# æ¨è–¦é…ç½®
PLANNER_LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.1  # éœ€è¦ä¸€è‡´æ€§

# ç”¨é€”: åˆ†ææŸ¥è©¢æ„åœ–ã€åˆ†é¡æŸ¥è©¢é¡å‹ã€ç”ŸæˆåŸ·è¡Œè¨ˆåŠƒ
# è€ƒé‡: ä½temperatureç¢ºä¿è¨ˆåŠƒçš„ç¢ºå®šæ€§
```

#### **2. Query Parser (æŸ¥è©¢è§£æ)**
```python
# æ¨è–¦é…ç½®
QUERY_PARSER_LLM_MODEL=gpt-4o
QUERY_PARSER_TEMPERATURE=0.1  # éœ€è¦ç²¾ç¢ºè§£æ

# ç”¨é€”: å°‡è‡ªç„¶èªè¨€è½‰ç‚ºçµæ§‹åŒ–JSON
# è€ƒé‡: éœ€è¦ç†è§£è¤‡é›œèªæ„ï¼Œå»ºè­°ä½¿ç”¨è¼ƒå¼·æ¨¡å‹
```

#### **3. Reasoning Agent (æ¨ç†åˆ†æ)**
```python
# æ¨è–¦é…ç½®
REASONER_LLM_MODEL=gpt-4o-mini
LLM_TEMPERATURE=0.1  # éœ€è¦é‚è¼¯ä¸€è‡´æ€§

# ç”¨é€”: çŸ¥è­˜åœ–è­œæ¨ç†ã€é—œä¿‚åˆ†æ
# è€ƒé‡: å¹³è¡¡æ€§èƒ½èˆ‡æº–ç¢ºæ€§
```

#### **4. Answerer (ç­”æ¡ˆç”Ÿæˆ)**
```python
# æ¨è–¦é…ç½®
ANSWERER_LLM_MODEL=gpt-4
ANSWERER_TEMPERATURE=0.3  # å…è¨±ä¸€å®šå‰µé€ æ€§

# ç”¨é€”: åŸºæ–¼è­‰æ“šç”Ÿæˆæœ€çµ‚ç­”æ¡ˆ
# è€ƒé‡: éœ€è¦é«˜å“è³ªå›æ‡‰ï¼Œç¨å¾®æé«˜temperatureä»¥å¢åŠ è‡ªç„¶åº¦
```

### é…ç½®é¸æ“‡æŒ‡å—

| å ´æ™¯ | æ¨è–¦æ¨¡å‹ | Temperature | è€ƒé‡é» |
|-----|---------|-------------|--------|
| **é–‹ç™¼æ¸¬è©¦** | gpt-3.5-turbo | 0.1 | æˆæœ¬æ•ˆç›Š |
| **ç”Ÿç”¢ç’°å¢ƒ** | gpt-4o-mini | 0.1 | æ€§èƒ½å¹³è¡¡ |
| **è¤‡é›œè§£æ** | gpt-4o | 0.1 | é«˜æº–ç¢ºæ€§ |
| **ç­”æ¡ˆç”Ÿæˆ** | gpt-4 | 0.3 | é«˜å“è³ª |

## ğŸ§ª LLM é…ç½®æ¸¬è©¦

### æ¸¬è©¦ LLM é€£ç·š

#### **é€£ç·šæ¸¬è©¦**
```python
# scripts/test_llm_connectivity.py
from grag.core.llm_factory import LLMFactory

async def test_connectivity():
    result = LLMFactory.validate_llm_connectivity()
    print(f"Status: {result['status']}")
    print(f"Available models: {result['models_available']}")

    if result['status'] != 'operational':
        print("Errors:", result['errors'])
```

#### **é‹è¡Œæ¸¬è©¦**
```bash
# æ¸¬è©¦LLMé…ç½®
uv run python scripts/test_llm_connectivity.py
```

### æ•ˆèƒ½æ¸¬è©¦

#### **Token ä½¿ç”¨é‡ç›£æ§**
```python
# ç›£æ§LLMèª¿ç”¨
with get_openai_callback() as cb:
    result = await agent.query("Test query")
    print(f"Tokens used: {cb.total_tokens}")
    print(f"Cost: ${cb.total_cost}")
```

## ğŸ” é™¤éŒ¯èˆ‡é©—è­‰

### æª¢æŸ¥ç•¶å‰é…ç½®
```python
from grag.core.config import settings

# æŸ¥çœ‹æ‰€æœ‰è¨­å®šå€¼
print(settings.dict())

# æª¢æŸ¥ç‰¹å®šå€¼ä¾†æº
import os
print(f"NEO4J_URI from env: {os.getenv('NEO4J_URI')}")
print(f"Settings value: {settings.neo4j_uri}")
```

### é©—è­‰é…ç½®è¼‰å…¥
```bash
# æ¸¬è©¦ç’°å¢ƒè®Šæ•¸è¦†è“‹
export CHUNK_SIZE=2000
python -c "from grag.core.config import settings; print(settings.chunk_size)"

# æª¢æŸ¥ .env æ–‡ä»¶
python -c "import os; print('DEBUG:', os.getenv('DEBUG', 'not set'))"
```

## ğŸ“ ç¸½çµ

é€™å€‹ä¸‰å±¤é…ç½®ç³»çµ±æä¾›äº†ï¼š

1. **éˆæ´»æ€§**: ç’°å¢ƒè®Šæ•¸å…è¨±é‹è¡Œæ™‚èª¿æ•´
2. **å®‰å…¨æ€§**: æ•æ„Ÿè³‡è¨Šä¸é€²å…¥ç‰ˆæœ¬æ§åˆ¶
3. **å¯ç¶­è­·æ€§**: å¸¸æ•¸é›†ä¸­ç®¡ç†ï¼Œé è¨­å€¼çµ±ä¸€
4. **é¡å‹å®‰å…¨**: Pydantic æä¾›é©—è­‰å’Œè½‰æ›
5. **é–‹ç™¼å‹å¥½**: æ¸…æ¥šçš„å„ªå…ˆç´šå’Œè¦†è“‹æ©Ÿåˆ¶
6. **LLM æ”¯æŒ**: é›†ä¸­å¼LLMé…ç½®ç®¡ç†ï¼Œæ”¯æŒå¤šAgentå°ˆç”¨æ¨¡å‹

### ç‰¹åˆ¥é‡å° LLM é…ç½®ï¼š

- **å¤šæ¨¡å‹æ”¯æŒ**: æ”¯æŒ OpenAIã€Ollama ç­‰å¤šç¨®æä¾›å•†
- **Agent å„ªåŒ–**: å„ Agent ä½¿ç”¨æœ€é©åˆçš„æ¨¡å‹å’Œåƒæ•¸
- **æˆæœ¬æ§åˆ¶**: æ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡åˆé©æ¨¡å‹
- **æ€§èƒ½ç›£æ§**: Token ä½¿ç”¨é‡å’Œæˆæœ¬è¿½è¹¤
- **æ¸¬è©¦é©—è­‰**: å®Œæ•´çš„é€£ç·šå’Œé…ç½®æ¸¬è©¦å·¥å…·

---

*é…ç½®ç³»çµ±è¨­è¨ˆéµå¾ª 12-Factor App åŸå‰‡ï¼Œæ”¯æ´é›²åŸç”Ÿéƒ¨ç½²å’Œå®¹å™¨åŒ–ç’°å¢ƒ*

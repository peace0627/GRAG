#!/usr/bin/env python3
"""
æ¸¬è©¦ Ollama å’Œ Gemma 3 Vision é€£æ¥çš„ç°¡å–®è…³æœ¬
"""

import asyncio
import os
from dotenv import load_dotenv
import ollama

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv()

async def test_ollama_connection():
    """æ¸¬è©¦ Ollama é€£æ¥å’ŒåŸºæœ¬åŠŸèƒ½"""
    try:
        print("ğŸ” æ¸¬è©¦ Ollama é€£æ¥...")

        # æª¢æŸ¥ Ollama æ˜¯å¦é‹è¡Œ
        client = ollama.Client(host=os.getenv('OLLAMA_HOST', 'http://localhost:11434'))

        # åˆ—å‡ºå¯ç”¨æ¨¡å‹
        models = client.list()
        print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹çµæ§‹: {type(models)}")

        try:
            model_list = models.models if hasattr(models, 'models') else models
            print(f"ğŸ“‹ å¯ç”¨æ¨¡å‹: {[m.model if hasattr(m, 'model') else str(m) for m in model_list]}")

            # æª¢æŸ¥æ˜¯å¦æœ‰ Gemma 3 æ¨¡å‹
            gemma_models = [m for m in model_list if hasattr(m, 'model') and 'gemma3' in m.model]
        except Exception as e:
            print(f"è§£ææ¨¡å‹åˆ—è¡¨å¤±æ•—: {e}")
            gemma_models = []

        if not gemma_models:
            print("âŒ æœªæ‰¾åˆ° Gemma 3 æ¨¡å‹")
            return False

        print(f"âœ… æ‰¾åˆ° Gemma 3 æ¨¡å‹: {[m.model for m in gemma_models]}")

        # æ¸¬è©¦åŸºæœ¬æ–‡å­—æ¨ç†
        print("\nğŸ§ª æ¸¬è©¦åŸºæœ¬æ–‡å­—æ¨ç†...")
        model_name = gemma_models[0].model

        response = client.generate(
            model=model_name,
            prompt="è«‹ç”¨ä¸­æ–‡å›ç­”ï¼šä½ æ˜¯ä¸€å€‹é†«æè²¡å ±åˆ†æåŠ©æ‰‹ï¼Œè«‹ç°¡çŸ­ä»‹ç´¹è‡ªå·±ã€‚",
            options={'temperature': 0.1}
        )

        print(f"ğŸ¤– æ¨¡å‹å›æ‡‰: {response['response'][:200]}...")

        print("âœ… Ollama å’Œ Gemma 3 åŸºæœ¬åŠŸèƒ½æ¸¬è©¦é€šé")
        return True

    except Exception as e:
        print(f"âŒ æ¸¬è©¦å¤±æ•—: {str(e)}")
        return False

async def test_vision_capability():
    """æ¸¬è©¦è¦–è¦ºèƒ½åŠ›ï¼ˆå¦‚æœæ”¯æ´ï¼‰"""
    try:
        print("\nğŸ–¼ï¸ æ¸¬è©¦è¦–è¦ºèƒ½åŠ›...")

        client = ollama.Client(host=os.getenv('OLLAMA_HOST', 'http://localhost:11434'))

        # æª¢æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æ´è¦–è¦ºï¼ˆé€šéæ¨¡æ¿åˆ¤æ–·ï¼‰
        models = client.list()
        model_list = models.models if hasattr(models, 'models') else models
        gemma_models = [m for m in model_list if hasattr(m, 'model') and 'gemma3' in m.model]

        if not gemma_models:
            return False

        model_name = gemma_models[0].model

        # æ¸¬è©¦æ˜¯å¦æ”¯æ´åœ–ç‰‡è¼¸å…¥ï¼ˆç°¡å–®çš„æ–‡æœ¬æç¤ºï¼‰
        test_prompt = "æè¿°ä¸€ä¸‹ä½ çœ‹åˆ°çš„åœ–ç‰‡ã€‚å¦‚æœçœ‹ä¸åˆ°åœ–ç‰‡ï¼Œè«‹èªª'æˆ‘çœ‹ä¸åˆ°åœ–ç‰‡'ã€‚"

        response = client.generate(
            model=model_name,
            prompt=test_prompt,
            options={'temperature': 0.1}
        )

        if "çœ‹ä¸åˆ°" in response['response'] or "ä¸èƒ½" in response['response']:
            print("â„¹ï¸ æ¨¡å‹ä¼¼ä¹ä¸æ”¯æ´ç•¶å‰ä¸Šä¸‹æ–‡ä¸­çš„è¦–è¦ºè¼¸å…¥ï¼ˆé€™æ˜¯æ­£å¸¸çš„ï¼‰")
        else:
            print(f"ğŸ¤– è¦–è¦ºæ¸¬è©¦å›æ‡‰: {response['response'][:100]}...")

        print("âœ… è¦–è¦ºèƒ½åŠ›æ¸¬è©¦å®Œæˆ")
        return True

    except Exception as e:
        print(f"âŒ è¦–è¦ºæ¸¬è©¦å¤±æ•—: {str(e)}")
        return False

if __name__ == "__main__":
    print("ğŸš€ é–‹å§‹ Ollama å’Œ Gemma 3 ç’°å¢ƒæª¢æŸ¥\n")

    # æ¸¬è©¦åŸºæœ¬é€£æ¥
    success1 = asyncio.run(test_ollama_connection())

    # æ¸¬è©¦è¦–è¦ºèƒ½åŠ›
    success2 = asyncio.run(test_vision_capability())

    if success1 and success2:
        print("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼ç’°å¢ƒæº–å‚™å®Œæˆã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ Ollama è¨­ç½®ã€‚")
